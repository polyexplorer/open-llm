{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bpw9cfg2415x",
        "q0xq43o56XIc",
        "fehUYyDm6Aex"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhdcMXqXAf+mgXKY7Hepyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polyexplorer/open-llm/blob/main/RAG%2BEval(Llama_Index).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "bpw9cfg2415x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urg3p_p0zZ2q"
      },
      "outputs": [],
      "source": [
        "! pip install transformers optimum accelerate langchain llama_index sentence_transformers peft trulens-eval\n",
        "! pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n",
        "! pip install pypdf pymupdf chromadb InstructorEmbedding\n",
        "! mkdir pdfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface LLM (Integrated with LlamaIndex)"
      ],
      "metadata": {
        "id": "YiBHDiDX45Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import TextStreamer, pipeline\n",
        "import torch\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                            #  trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "# pipe = pipeline(\n",
        "#             \"text-generation\",\n",
        "#             model=model,\n",
        "#             tokenizer=tokenizer,\n",
        "#             max_new_tokens=200,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.1,\n",
        "#             top_k=40,\n",
        "#             top_p=0.95,\n",
        "#             repetition_penalty=1.15,\n",
        "#             streamer=streamer,\n",
        "#         )\n",
        "\n",
        "# langchain_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 128,\n",
        "    query_wrapper_prompt = \"\"\"<|user|>\n",
        "{query_str}</s>\n",
        "<|assistant|>\n",
        "\"\"\",\n",
        "    system_prompt = \"\"\"\n",
        "You are a good Q/A chatbot who always answers the question based on the context only.</s>\"\"\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR15JBGn40tg",
        "outputId": "5ba22599-7490-423a-e9a6-8b9393876864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG Pipeline"
      ],
      "metadata": {
        "id": "DRLPfRM74mWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "uO2OAgG358gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Upload"
      ],
      "metadata": {
        "id": "q0xq43o56XIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn, content in uploaded.items():\n",
        "  filename = os.path.join(\"pdfs\",fn)\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Vng671mr6a9J",
        "outputId": "8a11a5f6-4b86-4eea-81dd-1d09bbc67251"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8cace373-913d-4632-bae1-71fa93a524bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8cace373-913d-4632-bae1-71fa93a524bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 263-102-00006_Protocol_Amendment_1_14Nov2019.pdf to 263-102-00006_Protocol_Amendment_1_14Nov2019 (2).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DocumentStore"
      ],
      "metadata": {
        "id": "_NI4rK6K5r83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import Document,SimpleDirectoryReader\n",
        "import os\n",
        "# filename = \"263-102-00006_Protocol_Amendment_1_14Nov2019 (1).pdf\"\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[filename]\n",
        ").load_data()\n",
        "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
      ],
      "metadata": {
        "id": "QWMIhoA84skW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "fehUYyDm6Aex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VectorStore Embeddings\n",
        "from llama_index.embeddings import InstructorEmbedding\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# model_name = \"AnnaWegmann/Style-Embedding\"\n",
        "model_name = \"hkunlp/instructor-large\"\n",
        "text_instruction = \"Represent the Medical document for retrieving important points where answer can be found:\"\n",
        "query_instruction = \"Represent the Medical question for retrieving supporting documents:\"\n",
        "\n",
        "embed_model = InstructorEmbedding(\n",
        "    model_name= model_name,\n",
        "    text_instruction=text_instruction,\n",
        "    query_instruction=query_instruction\n",
        "    )\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gl-_XQH4shc",
        "outputId": "fdf784be-953f-48df-e85f-314a0c23e6ad"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index"
      ],
      "metadata": {
        "id": "peGJuM_06E9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r sentence_index"
      ],
      "metadata": {
        "id": "57F_DCH7KrMJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Window Index\n",
        "\n",
        "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
        "from llama_index import load_index_from_storage\n",
        "import os\n",
        "\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(documents,\n",
        "#                                         service_context=service_context)\n",
        "\n",
        "def build_sentence_window_index(\n",
        "    documents, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
        "):\n",
        "    # create the sentence window node parser w/ default settings\n",
        "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "        window_size=2,\n",
        "        window_metadata_key=\"window\",\n",
        "        original_text_metadata_key=\"original_text\",\n",
        "    )\n",
        "    sentence_context = ServiceContext.from_defaults(\n",
        "        llm=llm,\n",
        "        embed_model=embed_model,\n",
        "        node_parser=node_parser,\n",
        "    )\n",
        "    if not os.path.exists(save_dir):\n",
        "        sentence_index = VectorStoreIndex.from_documents(\n",
        "            documents, service_context=sentence_context\n",
        "        )\n",
        "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
        "    else:\n",
        "        sentence_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=save_dir),\n",
        "            service_context=sentence_context,\n",
        "        )\n",
        "\n",
        "    return sentence_index\n",
        "\n",
        "def get_sentence_window_query_engine(\n",
        "    sentence_index,\n",
        "    similarity_top_k=6,\n",
        "    rerank_top_n=2,\n",
        "):\n",
        "    # define postprocessors\n",
        "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    rerank = SentenceTransformerRerank(\n",
        "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
        "    )\n",
        "\n",
        "    sentence_window_engine = sentence_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
        "    )\n",
        "    return sentence_window_engine\n",
        "\n",
        "\n",
        "sentence_index = build_sentence_window_index(documents=documents,llm=llm, save_dir=\"sentence_index\")\n"
      ],
      "metadata": {
        "id": "kqWIVhuqIQUy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oE3fXbAjIYfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retreival"
      ],
      "metadata": {
        "id": "qF17uD5A6JKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intialize RAG Pipeline"
      ],
      "metadata": {
        "id": "KG9ZzviW-3C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline = get_sentence_window_query_engine(sentence_index)"
      ],
      "metadata": {
        "id": "AEbpG4whzpPt"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q/A"
      ],
      "metadata": {
        "id": "PCoLwcSOyxc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = rag_pipeline.query(\"How many number of Site(s) will this trial take place on?\")\n",
        "print(str(response_1))"
      ],
      "metadata": {
        "id": "iFsQpEfA--aR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ed49f0-2be8-4e4c-de3c-ebb24e52b2bf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The context information provided indicates that this will be a single-site trial. Therefore, only one site will be involved in this trial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_2 = rag_pipeline.query(\"How many patients are planned to be taken for the trial?  \")\n",
        "print(str(response_2))"
      ],
      "metadata": {
        "id": "1go4jk-SzxFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d478bec1-d70c-42dd-bbd9-08c34cc81dc5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query asks how many patients are planned to be taken for the trial. Based on the context information provided, it can be seen that the trial population is planned to have at least 8 healthy male Japanese subjects who will receive the IV infusion, with a maximum of 10 subjects being dosed in total. Therefore, the answer to the query is that at least 8 and a maximum of 10 patients are planned to be taken for the trial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_2.source_nodes[0].node.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNFFoepoRePe",
        "outputId": "f7512aed-07c1-4d30-b8c9-1192548f41fe"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'window': '7) Positive alcohol breath test result or positive urine drug screen (confirmed by repeat) at Screening or Check -in. \\n 8) Positive hepatitis panel and/or positive hum an immunodeficiency virus test. \\n 9) Participation in a clinical trial involving administration of an investigational drug (new chemical entity  and/or OPC-61815) in the past  90 days prior to dosing or \\n5 half- lives of the investigational drug . \\n 10) Use or intend to use any medications/products known to alter drug absorption, metabolism, or elimination processes, including St. ',\n",
              " 'original_text': '9) Participation in a clinical trial involving administration of an investigational drug (new chemical entity  and/or OPC-61815) in the past  90 days prior to dosing or \\n5 half- lives of the investigational drug . \\n',\n",
              " 'page_label': '29',\n",
              " 'file_name': '263-102-00006_Protocol_Amendment_1_14Nov2019 (2).pdf',\n",
              " 'file_path': 'pdfs/263-102-00006_Protocol_Amendment_1_14Nov2019 (2).pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 895973,\n",
              " 'creation_date': '2023-12-04',\n",
              " 'last_modified_date': '2023-12-04',\n",
              " 'last_accessed_date': '2023-12-04'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_3 = rag_pipeline.query(\"What is the age of participants mentioned in the text? \")\n",
        "print(str(response_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdg2qDl0NGN_",
        "outputId": "0b4d28e3-0ba0-4dfc-fd77-fa656c82e334"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The participants mentioned in the text are male subjects between 35 and 55 years of age, as stated in the inclusion criteria mentioned in section 5.2.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_full = rag_pipeline.query(\"What are the demographics (age, gender, ethnicity etc) of the patients/subjects being taken for the trial?\")\n",
        "print(str(response_full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M00nGUnrQi62",
        "outputId": "f69b9899-43d0-4a7a-d060-3f0a019761d4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the context information provided, the demographic information (collection date, year of birth, age, sex, race, ethnicity, and country) will be recorded for all subjects at the screening visit (page_label: 28). However, the query does not specify whether the demographic information is for all subjects or just for the patients/subjects being taken for the trial. If we assume that the query refers to the subjects being taken for the trial, then the answer would be that the demographic information will be collected for at least 8 healthy male Japanese subjects who will be dosed to ensure that \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_4 = rag_pipeline.query(\"In an inpatient study, participants are admitted to a study site or are admitted to a clinic. In an inpatient study, the text also might mention subjects checking in and getting discharged. Is it an inpatient study? \")\n",
        "print(str(response_4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nayNjrTiNVn-",
        "outputId": "4e05adcd-2c4c-4c62-a4a0-777ea2ac21a7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context information provided, it is unclear whether this is an inpatient study or an outpatient study. The text mentions \"Final D discharge from trial will be day of discharge from residential treatment period or the day of last outpatient visit if required to attend them based on the discharge criteria.\" This suggests that some participants may be discharged from residential treatment, which could indicate an inpatient component, but it also mentions outpatient visits. Without further information, it is not possible to determine whether this is an inpatient or outpatient study.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_5 = rag_pipeline.query(\"In an outpatient study, participants visit the study site or must visit a clinic or must visit a hospital. In an outpatient study, participants do not stay overnight. Is it an outpatient study?   \")\n",
        "print(str(response_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8PMipjaNVlE",
        "outputId": "661b4bd4-f1e3-46f5-934e-8c6782a55d69"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, based on the context provided, it is an outpatient study as participants do not stay overnight and visits to the study site or clinic are required during nonresidential collection intervals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "htQv-vKzEJDy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIJCmWyGEQgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\"\"\"Are there sections mentioning Interim?\"\"\",\n",
        "\"\"\"Are there sections mentioning IA? \"\"\",\n",
        "\"\"\"How many sites are planned for the study? \"\"\",\n",
        "\"\"\"How many countries are planned for the study? \"\"\",\n",
        "\"\"\"Is a Non-USA country involved in the study? \"\"\"]\n",
        "\n",
        "from trulens_eval import Tru\n",
        "tru = Tru()\n",
        "\n",
        "tru.reset_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M08_UURUELP9",
        "outputId": "fdc740f0-e1c8-4177-c94c-ffc27b7da533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
            "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tru_recorder = get_prebuilt_trulens_recorder(query_engine,\n",
        "                                             app_id=\"Direct Query Engine\")"
      ],
      "metadata": {
        "id": "1qZI3dUSEos-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}