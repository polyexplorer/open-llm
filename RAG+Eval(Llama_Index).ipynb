{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polyexplorer/open-llm/blob/main/RAG%2BEval(Llama_Index).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpw9cfg2415x"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiBHDiDX45Gk"
      },
      "source": [
        "# Huggingface LLM (Integrated with LlamaIndex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR15JBGn40tg",
        "outputId": "ea04b9bd-0649-4779-96c5-398dc2543f3d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import TextStreamer, pipeline\n",
        "import torch\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model_name_or_path = \"TheBloke/neural-chat-7B-v3-2-GPTQ\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                            #  trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "def generate_response(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=256,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response_ids = outputs[0]\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            temperature=0.1,\n",
        "            top_k=40,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.15,\n",
        "            # streamer=streamer,\n",
        "        )\n",
        "\n",
        "# langchain_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "system_prompt = \"### System: You are a good Q/A chatbot who always answers the question based on the context only.\"\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### User:\n",
        "{query_str}\n",
        "\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "\n",
        "# prompt_template = \"GPT4 Correct User: {query_str}<|end_of_turn|>GPT4 Correct Assistant:\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 256,\n",
        "    query_wrapper_prompt = prompt_template,\n",
        "    system_prompt = system_prompt,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLPfRM74mWY"
      },
      "source": [
        "# Basic RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO2OAgG358gc"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xq43o56XIc"
      },
      "source": [
        "### File Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NI4rK6K5r83"
      },
      "source": [
        "### DocumentStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QWMIhoA84skW"
      },
      "outputs": [],
      "source": [
        "from llama_index import Document,SimpleDirectoryReader\n",
        "import os\n",
        "filename = \"/home/ubuntu/open-llm/pdfs/X06-201-00001_Protocol_Amendment_4\"\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[filename]\n",
        ").load_data()\n",
        "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fehUYyDm6Aex"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gl-_XQH4shc",
        "outputId": "5c96437c-d56b-4ca3-8963-6871d50d869d"
      },
      "outputs": [],
      "source": [
        "# VectorStore Embeddings\n",
        "from llama_index.embeddings import InstructorEmbedding, HuggingFaceEmbedding\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# model_name = \"AnnaWegmann/Style-Embedding\"\n",
        "# model_name = \"hkunlp/instructor-large\"\n",
        "# model_name = \"BAAI/bge-large-en-v1.5\"\n",
        "\n",
        "# text_instruction = \"Represent the Medical document for retrieving important points where answer can be found:\"\n",
        "# query_instruction = \"Represent the Medical question for retrieving supporting documents:\"\n",
        "\n",
        "# embed_model = InstructorEmbedding(\n",
        "#     model_name= model_name,\n",
        "#     text_instruction=text_instruction,\n",
        "#     query_instruction=query_instruction\n",
        "#     )\n",
        "\n",
        "# embed_model = HuggingFaceEmbedding(\n",
        "#     model_name= model_name\n",
        "#     )\n",
        "\n",
        "\n",
        "\n",
        "# service_context = ServiceContext.from_defaults(\n",
        "#     llm=llm, embed_model=embed_model\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peGJuM_06E9B"
      },
      "source": [
        "### Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "57F_DCH7KrMJ"
      },
      "outputs": [],
      "source": [
        "! rm -r sentence_index\n",
        "! rm -r merging_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kqWIVhuqIQUy"
      },
      "outputs": [],
      "source": [
        "# Auto-Merging Index\n",
        "\n",
        "from llama_index.node_parser import HierarchicalNodeParser\n",
        "\n",
        "from llama_index.node_parser import get_leaf_nodes\n",
        "from llama_index import StorageContext\n",
        "from llama_index.retrievers import AutoMergingRetriever\n",
        "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "\n",
        "def build_automerging_index(\n",
        "    documents,\n",
        "    llm,\n",
        "    embed_model=\"local:BAAI/bge-large-en-v1.5\",\n",
        "    save_dir=\"merging_index\",\n",
        "    chunk_sizes=None,\n",
        "):\n",
        "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
        "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
        "    nodes = node_parser.get_nodes_from_documents(documents)\n",
        "    leaf_nodes = get_leaf_nodes(nodes)\n",
        "    merging_context = ServiceContext.from_defaults(\n",
        "        llm=llm,\n",
        "        embed_model=embed_model,\n",
        "    )\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    storage_context.docstore.add_documents(nodes)\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        automerging_index = VectorStoreIndex(\n",
        "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
        "        )\n",
        "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
        "    else:\n",
        "        automerging_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=save_dir),\n",
        "            service_context=merging_context,\n",
        "        )\n",
        "    return automerging_index\n",
        "\n",
        "def get_automerging_query_engine(\n",
        "    automerging_index,\n",
        "    similarity_top_k=12,\n",
        "    rerank_top_n=2,\n",
        "):\n",
        "    # base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
        "    # retriever = AutoMergingRetriever(\n",
        "    #     base_retriever, automerging_index.storage_context, verbose=True\n",
        "    # )\n",
        "    rerank = SentenceTransformerRerank(\n",
        "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
        "    )\n",
        "    auto_merging_engine = automerging_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k,\n",
        "        node_postprocessors=[rerank]\n",
        "        )\n",
        "    return auto_merging_engine\n",
        "\n",
        "\n",
        "# Sentence Window Index\n",
        "\n",
        "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
        "from llama_index import load_index_from_storage\n",
        "import os\n",
        "\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(documents,\n",
        "#                                         service_context=service_context)\n",
        "\n",
        "def build_sentence_window_index(\n",
        "    documents, llm, embed_model=\"local:BAAI/bge-large-en-v1.5\", save_dir=\"sentence_index\"\n",
        "):\n",
        "    # create the sentence window node parser w/ default settings\n",
        "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "        window_size=2,\n",
        "        window_metadata_key=\"window\",\n",
        "        original_text_metadata_key=\"original_text\",\n",
        "    )\n",
        "    sentence_context = ServiceContext.from_defaults(\n",
        "        llm=llm,\n",
        "        embed_model=embed_model,\n",
        "        node_parser=node_parser,\n",
        "    )\n",
        "    if not os.path.exists(save_dir):\n",
        "        sentence_index = VectorStoreIndex.from_documents(\n",
        "            documents, service_context=sentence_context\n",
        "        )\n",
        "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
        "    else:\n",
        "        sentence_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=save_dir),\n",
        "            service_context=sentence_context,\n",
        "        )\n",
        "\n",
        "    return sentence_index\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_window_query_engine(\n",
        "    sentence_index,\n",
        "    similarity_top_k=6,\n",
        "    rerank_top_n=2,\n",
        "):\n",
        "    # define postprocessors\n",
        "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    rerank = SentenceTransformerRerank(\n",
        "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
        "    )\n",
        "\n",
        "    sentence_window_engine = sentence_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
        "    )\n",
        "    return sentence_window_engine\n",
        "\n",
        "\n",
        "sentence_index = build_sentence_window_index(documents=documents,llm=llm, save_dir=\"sentence_index\")\n",
        "automerging_index = build_automerging_index(\n",
        "    documents=documents,\n",
        "    llm=llm,\n",
        "    save_dir=\"merging_index\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "! rm -r sentence_index\n",
        "! rm -r merging_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index import Document,SimpleDirectoryReader\n",
        "import os\n",
        "\n",
        "def create_sentence_query_engine(pdf_path):\n",
        "    documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
        "    sentence_index = build_sentence_window_index(documents=documents,llm=llm, save_dir='sentence_index')\n",
        "    return get_sentence_window_query_engine(sentence_index)\n",
        "\n",
        "\n",
        "def create_automerging_query_engine(pdf_path):\n",
        "    documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
        "    automerging_index = build_automerging_index(documents=documents,llm=llm, save_dir='merging_index')\n",
        "    return get_automerging_query_engine(automerging_index,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF17uD5A6JKA"
      },
      "source": [
        "## Retreival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG9ZzviW-3C-"
      },
      "source": [
        "### Intialize RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8fb1e5a670> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8fb1e5a670> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8fb1e5a670> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8fb1e5a670> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a75430> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a75430> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a75430> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a3fc10> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
            "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Ground-Truth Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Ground-Truth Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "from trulens_eval.feedback import Groundedness\n",
        "from trulens_eval import Feedback, TruLlama\n",
        "from trulens_eval.feedback.provider.hugs import Huggingface\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "huggingface_provider = Huggingface()\n",
        "\n",
        "\n",
        "\n",
        "def convert_score(score):\n",
        "    if score > 1:\n",
        "        # Find the next highest power of 10\n",
        "        power = math.ceil(math.log10(score))\n",
        "        # Divide the score by 10 to the power\n",
        "        score /= 10 ** power\n",
        "    return float(score)\n",
        "\n",
        "pat_0_10 = re.compile(r\"\\s*([0-9]+)[\\s\\\\]*$\")\n",
        "\n",
        "def extract_first_number(s):\n",
        "    \"\"\"\n",
        "    Extracts the first number mentioned in a string.\n",
        "\n",
        "    :param s: A string that may contain numbers.\n",
        "    :return: The first number found in the string, or None if no number is found.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\d+', s)\n",
        "    return int(match.group()) if match else None\n",
        "\n",
        "def re_0_10_rating(str_val):\n",
        "    matches = pat_0_10.fullmatch(str_val)\n",
        "    if not matches:\n",
        "        # Try soft match\n",
        "        matches = re.search('([0-9]+)(?=\\D*$)', str_val)\n",
        "        if not matches:\n",
        "            print(f\"0-10 rating regex failed to match on: '{str_val}'\")\n",
        "            return -10  # so this will be reported as -1 after division by 10\n",
        "\n",
        "    return float(matches.group())\n",
        "\n",
        "def _extract_score_and_reasons_from_response(\n",
        "    response,\n",
        "    normalize = 10.0\n",
        "):\n",
        "  if \"Supporting Evidence\" in response:\n",
        "    score = 0.0\n",
        "    supporting_evidence = \"\"\n",
        "    for line in response.split('\\n'):\n",
        "      if \"Score\" in line:\n",
        "        score = re_0_10_rating(line) / normalize\n",
        "      if \"Criteria\" in line:\n",
        "        parts = line.split(\":\")\n",
        "        if len(parts) > 1:\n",
        "          criteria = \":\".join(parts[1:]).strip()\n",
        "      if \"Supporting Evidence\" in line:\n",
        "        parts = line.split(\":\")\n",
        "        if len(parts) > 1:\n",
        "          supporting_evidence = \":\".join(parts[1:]).strip()\n",
        "    reasons = {\n",
        "      'reason':\n",
        "          (\n",
        "            f\"{'Criteria: ' + str(criteria) + ' ' if criteria else ''}\\n\"\n",
        "            f\"{'Supporting Evidence: ' + str(supporting_evidence) if supporting_evidence else ''}\"\n",
        "          )\n",
        "    }\n",
        "    return score, reasons\n",
        "  else:\n",
        "    return re_0_10_rating(response) / normalize\n",
        "\n",
        "def llm_output_parser(prompt,response):\n",
        "    final_prompt = f'''### System: You are a good parser. If a user question and a user answer is given, convert it to a Python dictionary with a key named presence or number and value as True or False or a number depending on the user answer. Understand user question with user answer and convert it to a Python dictionary. Assistant has to figure out boolean or integer values of the dictionary based on the user answer. \n",
        "Below are some examples.\n",
        "\n",
        "User Question: Does this text mention Translational Medicine Research, which is a research approach that aims to 'translate' findings from fundamental research into medical practice and meaningful health outcomes? One of the examples is utilization of Neurocart.\n",
        "User Answer: The text does not mention Translational Medicine Research.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"presence\": False,\n",
        "}}\n",
        "\n",
        "User Question: How many patients are planned for the  study?\n",
        "User Answer: As per the given text, there are 88 patients planned for the study in total.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"number\":88,\n",
        "}} \n",
        "\n",
        "User Question: Does this text mention Translational Medicine Research, which is a research approach that aims to 'translate' findings from fundamental research into medical practice and meaningful health outcomes? One of the examples is utilization of Neurocart.\n",
        "User Answer: The text mentions Translational Medicine Research.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"presence\": True,\n",
        "}}\n",
        "User Question: How many countries will this trial take place over?\n",
        "User Answer: As per the given text, the trial will only be conducted in the UK.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"number\":1,\n",
        "}} \n",
        "\n",
        "\n",
        "User Question: Is it an oncology study?\n",
        "User Answer: The study text is a study for cancer. Therefore, it is an oncology study.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"presence\": True,\n",
        "}}\n",
        "\n",
        "User Question: What is the Age Range of the  Patients in the trial?\n",
        "User Answer: As per the given text, the patient ages will be 24-39, inclusive.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"min_number\":24,\n",
        "\"max_number\":39,\n",
        "}} \n",
        "\n",
        "User Question: Is there any mention of Observational period in the text?\n",
        "User Answer: I apologize but the information on the observational period is not mentioned in the text.\n",
        "Assistant Response:\n",
        "{{\n",
        "\"presence\": False,\n",
        "}}\n",
        "### User:\n",
        "User Question is {prompt} and User Answer is {response}.\n",
        "Without making assumptions or inferring meanings beyond the dataset description provided, convert User Answer to a Python Dictionary and give an Assistant Response for the user question-user answer pair\n",
        "If you dont know the answer, just say that you dont know, dont try to make up an answer. No explanation of the assistant response is required.\n",
        "### Assistant:\n",
        "    '''\n",
        "    response = pipe(final_prompt)[0]['generated_text'].split('### Assistant:')[-1]\n",
        "    return response\n",
        "\n",
        "\n",
        "def select_ground_truth(inp, questions_list):\n",
        "    for q_obj in questions_list:\n",
        "        if q_obj['question'] == inp:\n",
        "            return q_obj['ground_truth']    \n",
        "    \n",
        "    return \"<NOT AVAILABLE>\"\n",
        "\n",
        "def gt_relevance_with_cot_reasons(prompt,response):\n",
        "  ground_truth = select_ground_truth(prompt,questions_list)\n",
        "  parsed_response = llm_output_parser(prompt,response)\n",
        "  parsed_ground_truth = llm_output_parser(prompt,ground_truth)\n",
        "  final_prompt = f\"\"\"### System:\n",
        "\n",
        "You are an JSON DIFFERENCE CHECKER; providing whether the 'presence' or 'number' values in the given JSON strings are the same or not.\n",
        "\n",
        "Given two JSON strings, Please answer with this template:\n",
        "\n",
        "TEMPLATE FORMAT:\n",
        "REASONS: <reasoning for your answer.>\n",
        "SCORE: <The score between 0 and 10, based on similarity of values>\n",
        "\n",
        "Do not give high scores until absolutely sure. Even if the keys are same or different , only compare the values. \n",
        "\n",
        "### User:\n",
        "JSON STRINGS:\n",
        "First:\n",
        "{parsed_response}\n",
        "Second:\n",
        "{parsed_ground_truth }\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "  response = pipe(final_prompt)[0]['generated_text'].split('### Assistant:')[-1]\n",
        "  print(\"Response: \\n \",response)\n",
        "  score = convert_score(extract_first_number(response.split('SCORE')[-1]))\n",
        "  reasons = response.split('SCORE')[0].split('REASONS')[-1]\n",
        "  reasons =   {'reason':reasons}\n",
        "  # return score,reasons\n",
        "  return score,reasons\n",
        "\n",
        "\n",
        "\n",
        "def relevance_with_cot_reasons(prompt, response):\n",
        "\n",
        "  final_prompt = f\"\"\"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\n",
        "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n",
        "\n",
        "Please answer with this template:\n",
        "\n",
        "TEMPLATE FORMAT:\n",
        "Criteria: <The criteria for your evaluation>\n",
        "Supporting Evidence: <Your reasons for your scoring.>\n",
        "Score: <The score 0-10 based on the given criteria>\n",
        "\n",
        "A few additional scoring guidelines:\n",
        "\n",
        "- Long RESPONSES should score equally well as short RESPONSES.\n",
        "\n",
        "- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.\n",
        "\n",
        "- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\n",
        "\n",
        "- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
        "\n",
        "- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
        "\n",
        "- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\n",
        "\n",
        "- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
        "\n",
        "- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\n",
        "\n",
        "- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\n",
        "\n",
        "- RESPONSE that confidently FALSE should get a score of 0.\n",
        "\n",
        "- RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
        "\n",
        "PROMPT: {prompt}\n",
        "\n",
        "RESPONSE: {response}\n",
        "  \"\"\"\n",
        "  response = pipe(prompt_template.format(query_str=final_prompt))[0]['generated_text']\n",
        "  score,reasons = _extract_score_and_reasons_from_response(response)\n",
        "  score = convert_score(score)  \n",
        "  return score,reasons\n",
        "\n",
        "\n",
        "huggingface_provider = Huggingface()\n",
        "\n",
        "grounded = Groundedness(groundedness_provider=huggingface_provider)\n",
        "\n",
        "def groundedness_measure_with_cot_reasons(source, statement):\n",
        "  prompt_template = f\"\"\"### System:\n",
        "  You are a INFORMATION OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.For every sentence in the statement, please answer with this template:\n",
        "TEMPLATE:\n",
        "Statement Sentence: <Sentence>,\n",
        "Supporting Evidence: <Choose the exact unchanged sentences in the source that can answer the statement, if nothing matches, say NOTHING FOUND>\n",
        "Score: <Output a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping>\n",
        "\n",
        "### User:\n",
        "Give me the INFORMATION OVERLAP of this SOURCE and STATEMENT.\n",
        "SOURCE: {source}\n",
        "\n",
        "STATEMENT: {statement}</s>\n",
        "### Assistant\n",
        "\"\"\"\n",
        "  groundedness_scores = {}\n",
        "  plausible_junk_char_min = 4\n",
        "  if len(statement) > plausible_junk_char_min:\n",
        "    reason = pipe(prompt_template)[0]['generated_text'].split('### Assistant')[-1]\n",
        "  i = 0\n",
        "  for line in reason.split('\\n'):\n",
        "    if \"Score\" in line:\n",
        "      groundedness_scores[f\"statement_{i}\"] = re_0_10_rating(line) / 10\n",
        "      i += 1\n",
        "  for k,v in groundedness_scores.items():\n",
        "     groundedness_scores[k] = convert_score(v)\n",
        "  return groundedness_scores, {\"reason\": reason}\n",
        "\n",
        "\n",
        "groundedness = (\n",
        "    Feedback(groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
        "        .on(TruLlama.select_source_nodes().node.text)\n",
        "        .on_output()\n",
        "        .aggregate(grounded.grounded_statements_aggregator)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "qa_relevance = (\n",
        "    Feedback(relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
        "    .on_input_output()\n",
        ")\n",
        "\n",
        "gt_relevance = (\n",
        "    Feedback(gt_relevance_with_cot_reasons, name=\"Ground-Truth Relevance\")\n",
        "    .on_input_output()\n",
        ")\n",
        "\n",
        "qs_relevance = (\n",
        "    Feedback(relevance_with_cot_reasons, name = \"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(TruLlama.select_source_nodes().node.text)\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "feedbacks = [qa_relevance, qs_relevance,gt_relevance, groundedness]\n",
        "\n",
        "\n",
        "def get_prebuilt_trulens_recorder(query_engine, feedbacks, app_id):\n",
        "    tru_recorder = TruLlama(\n",
        "        query_engine,\n",
        "        app_id=app_id,\n",
        "        feedbacks=feedbacks\n",
        "        )\n",
        "    return tru_recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trulens_eval import Tru, TruLlama\n",
        "tru = Tru()\n",
        "# tru.reset_database()\n",
        "\n",
        "def run_evals(filename, questions_list):\n",
        "    tot = len(questions_list)\n",
        "\n",
        "    print(f\"Creating RAG Pipelines for document {filename.split('/')[-1]}...\")\n",
        "    sentence_pipeline = create_sentence_query_engine(filename)\n",
        "    print(\"Created Sentence Window RAG.\")\n",
        "    automerging_pipeline = create_automerging_query_engine(filename)\n",
        "    print(\"Created Auto-Merging RAG.\")\n",
        "\n",
        "    app_name = f\"RAG - {filename.split('/'[-1])}\"\n",
        "\n",
        "    sentence_recorder = get_prebuilt_trulens_recorder(sentence_pipeline,feedbacks, app_id = f\"Sentence Window {app_name}\")\n",
        "    automerging_recorder = get_prebuilt_trulens_recorder(automerging_pipeline,feedbacks, app_id = f\"Auto-Merging {app_name}\")\n",
        "    \n",
        "    print(\"Evaluating on Questions List:\")\n",
        "    for i,question in enumerate(questions_list):\n",
        "        with sentence_recorder as recording:\n",
        "            response = sentence_pipeline.query(question)\n",
        "        print(f\"Sentence Window RAG ({i+1}/{tot})\")\n",
        "        with automerging_recorder as recording:\n",
        "            response = automerging_pipeline.query(question)\n",
        "        print(f\"Auto-Merging RAG ({i+1}/{tot})\")\n",
        "    \n",
        "    del sentence_pipeline, automerging_pipeline, sentence_recorder, automerging_recorder\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AEbpG4whzpPt"
      },
      "outputs": [],
      "source": [
        "filename = \"/home/ubuntu/open-llm/pdfs/X06-201-00001_Protocol_Amendment_4\"\n",
        "questions_list  = [\n",
        "    {\n",
        "        'question':\"Does this trial involve Psychedelic Research, which is the study of the effects of psychedelic substances, like LSD and psilocybin, on the human brain and mental health?\",\n",
        "        'ground_truth':\"To answer your question, the text does not mention anything about psychedelic research or the use of substances like LSD and psilocybin. The trial is focused on the investigation of B-124a, an orally bioavailable NAM of the NMDA receptor with conferred selectivity to the NR2B subunit, and its effects on blood pressure, quantitative electroencephalogram (qEEG) measures, and neurological scale scores.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Patients, subjects, and participants are used interchangeably. Other synonyms are Enrollees, study volunteers, research recruits, cohort members, survey respondents. How many patients are planned for study according to the text?\",\n",
        "        'ground_truth':\"Regarding your question, Arm 1 is a randomized, double-blind, placebo-controlled, single ascending oral dose administration of B-124a to healthy subjects in a fasted state where dosing is planned to be conducted in 7 cohorts of 8 subjects each.  Therefore, for Arm 1, 56  subjects are planned. Arm 2 is a randomized, double-blind, placebo-controlled, single ascending oral dose administration of B-124a to healthy subjects in a fasted state where dosing is planned to be conducted in 3 or 4 cohorts of 8 subjects each.  Therefore, for Arm 2, 32 subjects are planned. So, in total, 88 subjects are planned for the study \",\n",
        "    },\n",
        "    {\n",
        "        'question':\"What is the age of participants mentioned in the text?\",\n",
        "        'ground_truth':\"The age of participants mentioned in the text is between 18 and 55 years of age, inclusive. This information can be found on Page 33 of the protocol.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Are Adverse Events of Special Interest mentioned and applicable for the study? They are sometimes abbreviated as AESI or AEs of Special Interest.\",\n",
        "        'ground_truth':\"To answer your question, Adverse Events of Special Interest (AESIs) is mentioned in the study but not applicable.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"What is the treatment period mentioned in the study?\",\n",
        "        'ground_truth':\"The treatment period is defined as the time period during which subjects are evaluated for primary and/or secondary objectives of the trial irrespective of whether or not the subject actually consumes all doses of the IMP. Subjects who are evaluated at the last scheduled visit during the treatment period will be defined as trial completers. For purposes of this trial, subjects who complete all PK and PD assessments all the way through discharge from the clinic, assuming PK washout and scheduled in-clinic AE assessment completion (ie, complete the Day 8 visit, will be defined as trial completers.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"In an inpatient study, participants are admitted to a study site or are admitted to a clinic. In an inpatient study, the text also might mention subjects checking in and getting discharged. Is it an inpatient study?\",\n",
        "        'ground_truth':\"It appears that this is an inpatient study . The trial will consist of a screening period (Day ?45 through Day ?2), check-in (Day ?1), in-clinic stay (minimum of 8 days), and a safety follow-up telephone call 30 (+ 2) days after the last dose of B-124a (the investigational medicinal product [IMP]) to assess any new or ongoing adverse events (AEs) and to record concomitant medications. \",\n",
        "    },\n",
        "    {\n",
        "        'question':\"In an outpatient study, participants visit the study site or must visit a clinic or must visit a hospital. In an outpatient study, participants do not stay overnight. Is it an outpatient study?\",\n",
        "        'ground_truth':\"It appears that this is not an outpatient study, but an inpatient study.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"How many trial centers or sites are planned for the study?\",\n",
        "        'ground_truth':\"To answer your question, there is a single site planned for the study\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Is this trial Placebo-controlled or an open trial?\",\n",
        "        'ground_truth':\"To answer your question, the trial is a randomized, double-blind, placebo-controlled trial.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Is Food effect mentioned in this text? Food effect describes how the presence or absence of food in the stomach can affect the rate and extent to which a drug is absorbed into the bloodstream. It is sometimes abbreviated as FE.\",\n",
        "        'ground_truth':\"To answer your question, yes, the effect of food on B-124a safety, tolerability, and PK will be determined in this trial.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Are Healthy adult participants a part of this trial?\",\n",
        "        'ground_truth':\"To answer your question, yes, healthy adult participants are a part of this trial. The trial population will consist of healthy males and females, 18 to 55 years of age, inclusive. Approximately 88 subjects (56 subjects in Arm 1 and 32 subjects in Arm 2) are expected to be enrolled in the trial.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Is it a double blind randomization clinical study?\",\n",
        "        'ground_truth':\"To answer your question, yes, the study is a double-blind randomized clinical study. For each cohort in Arm 1 and Arm 2, subjects will be randomized on Day 1 to a single oral dose of B-124a or matching placebo in a 6:2 ratio (6 on B-124a subjects and 2 placebo subjects)\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"How many total number of drugs are being assessed as Investigational Medicinal Product in this study?\",\n",
        "        'ground_truth':\"To answer your question, there is only one investigational medicinal product being assessed in this study, which is B-124a. This is mentioned in the protocol summary on page 1 and in various sections throughout the document.\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Is dose administration mentioned in text of treatment section? Other synonyms of dose administration are IMP administration and study medication.\",\n",
        "        'ground_truth':\"\"\"To answer your question, yes, the text mentions dose administration in several places. Synonyms used include IMP administration and study medication. For example, it states that \"all doses of IMP will be administered while the subjects are in the clinic.\" Additionally, it mentions that the IMP will be supplied as API and will be labeled with instructions for use and route of administration.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        'question':\"Are the lab samples sent to one central lab or a different location? Lab samples are sometimes referred to as biomarkers, FBR, Fasting Blood referrals or bodily fluid samples.\",\n",
        "        'ground_truth':\"\"\"To answer your question, the lab samples collected for clinical laboratory assessments in this trial will be sent to a local laboratory. The protocol does not use the terms biomarkers, FBR, Fasting Blood referrals, or bodily fluid samples to refer to the lab samples collected\"\"\",\n",
        "    },\n",
        "    \n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sentence_pipeline = create_sentence_query_engine(filename)\n",
        "automerging_pipeline = create_automerging_query_engine(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sx8L0269PQXP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    \n",
            "{\n",
            "\"presence\": False,\n",
            "}\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCoLwcSOyxc-"
      },
      "source": [
        "### Q/A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFsQpEfA--aR",
        "outputId": "81840537-26dd-4feb-fa74-0abbd404f4b6"
      },
      "outputs": [],
      "source": [
        "# response_sentence = sentence_pipeline.query(\"Does the study title mention that it is about monitoring Drug-Drug Interactions? Drug drug Interactions occur when two or more drugs interact with each other in a way that affects their effectiveness or safety It is sometimes abbreviated as DDI.  \")\n",
        "# print(str(response_sentence), \"\\n-----------------------------------\\n\")\n",
        "\n",
        "# response_automerging = automerging_pipeline.query(\"Does the study title mention that it is about monitoring Drug-Drug Interactions? Drug drug Interactions occur when two or more drugs interact with each other in a way that affects their effectiveness or safety It is sometimes abbreviated as DDI.  \")\n",
        "# print(str(response_automerging))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1go4jk-SzxFz",
        "outputId": "5a42ba52-0b86-44d9-d08b-3a6daf0cb7c3"
      },
      "outputs": [],
      "source": [
        "# response_2 = sentence_pipeline.query(\"How many patients are planned to be taken for the trial?  \")\n",
        "# print(str(response_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNFFoepoRePe",
        "outputId": "ed2e38a1-a7de-49b7-ce49-36b5cf7bdd09"
      },
      "outputs": [],
      "source": [
        "# response_2.source_nodes[0].node.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdg2qDl0NGN_",
        "outputId": "995ac7db-0319-47b7-c285-082b6db3d449"
      },
      "outputs": [],
      "source": [
        "# response_3 = sentence_pipeline.query(\"What is the age of participants mentioned in the text? \")\n",
        "# print(str(response_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M00nGUnrQi62",
        "outputId": "ac318867-a3e9-4fd5-fb46-dafd3c7acc14"
      },
      "outputs": [],
      "source": [
        "# response_full = sentence_pipeline.query(\"What are the demographics (age, gender, ethnicity etc) of the patients/subjects being taken for the trial?\")\n",
        "# print(str(response_full))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nayNjrTiNVn-",
        "outputId": "62cbe779-061c-43ba-9079-d20bfb5bb5a1"
      },
      "outputs": [],
      "source": [
        "# response_4 = sentence_pipeline.query(\"In an inpatient study, participants are admitted to a study site or are admitted to a clinic. In an inpatient study, the text also might mention subjects checking in and getting discharged. Is it an inpatient study? \")\n",
        "# print(str(response_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8PMipjaNVlE",
        "outputId": "0978034c-8cbf-4843-de61-8da7afe8124c"
      },
      "outputs": [],
      "source": [
        "# response_5 = sentence_pipeline.query(\"In an outpatient study, participants visit the study site or must visit a clinic or must visit a hospital. In an outpatient study, participants do not stay overnight. Is it an outpatient study?   \")\n",
        "# print(str(response_5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htQv-vKzEJDy"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_questions = [\"\"\"What is the total number of study arms the study is dealing with? \"\"\"\n",
        ",\"\"\"What is the total number of cohorts the study is dealing with? \"\"\"\n",
        ",\"\"\"What is the total number of treatment groups the study is dealing with? \"\"\"\n",
        ",\"\"\"What is the total number of treatment sequences the study is dealing with? \"\"\"\n",
        ",\"\"\"Does this text mention Translational Medicine Research, which is a research approach that aims to 'translate' findings from fundamental research into medical practice and meaningful health outcomes? One of the examples is utilization of Neurocart. \"\"\"\n",
        ",\"\"\"Does this trial involve Psychedelic Research, which is the study of the effects of psychedelic substances, like LSD and psilocybin, on the human brain and mental health? \"\"\"\n",
        ",\"\"\"Does this trial mention Abuse Liability, which refers to the potential of a drug to be misused, leading to addiction or dependence? It is also sometimes referred to as Human Abuse Liability or HAL. \"\"\"\n",
        ",\"\"\"Is this a Basket trial, which is a clinical trial design where multiple subgroups (baskets) of patients, usually with different types of cancer, are tested with a single drug based on a common biomarker? \"\"\"\n",
        ",\"\"\"Is this an Umbrella trial, a type of clinical trial that tests the impact of different drugs on different mutations in a single type of disease, usually cancer, in one 'umbrella' study? \"\"\"\n",
        ",\"\"\"Is 'First in Human' mentioned in this text, which refers to the first time a new treatment or procedure is tested in humans? It is also sometimes abbreviated as FIH. \"\"\"\n",
        ",\"\"\"Is it an oncology study? \"\"\"\n",
        ",\"\"\"Is 'Single Ascending Dose' mentioned in this text, referring to a phase in clinical trials where the dosage is gradually increased to evaluate the body's reactions? It is sometimes abbreviated as SAD. \"\"\"\n",
        ",\"\"\"Is 'Multiple Ascending Dose' mentioned in this text, which refers to a method in clinical trials where small groups of subjects receive multiple low doses of the drug, which are gradually increased? It is sometimes abbreviated as MAD. \"\"\"\n",
        ",\"\"\"Is 'Thorough QTc' mentioned in this text, referring to a clinical trial design used to assess the impact of a drug on the heart's QT interval, which is a measure of the time between the start of the Q wave and the end of the T wave in the heart's electrical cycle? \"\"\"\n",
        ",\"\"\"Is there any mention of run in in the text? \"\"\"\n",
        ",\"\"\"During the run in period, participants undergo specific procedures to establish baseline measurements or assess eligibility criteria. Does the study involve run in period? It is also sometimes called washout period or lead-in period. \"\"\"\n",
        ",\"\"\"Is there any mention of Observational period in the text?  \"\"\"\n",
        ",\"\"\"Observational period refers where researchers observe participants and collect various types of data including medical history, symptoms, outcomes, biomarkers etc. Does the study involve an Observational period? \"\"\"\n",
        ",\"\"\"Is there any mention of titration period in full text? \"\"\"\n",
        ",\"\"\"Titration period is when the dosage of a medication is gradually adjusted until an optimal dose is reached. Does the study involve a titration period? \"\"\"\n",
        ",\"\"\"Is the trial investigating the drug’s bioavailability? Bioavailability represents the extent and rate at which a drug is absorbed. It is sometimes abbreviated as BA. \"\"\"\n",
        ",\"\"\"Is the trial investigating bioequivalence of drugs? Bioequivalence is the similarity in the rate and extent of drug absorption between two drug products, typically a generic and a brand-name drug. It is sometimes abbreviated as BE. \"\"\"\n",
        ",\"\"\"Does the study title mention that it is about monitoring Drug-Drug Interactions? Drug drug Interactions occur when two or more drugs interact with each other in a way that affects their effectiveness or safety It is sometimes abbreviated as DDI.  \"\"\"\n",
        ",\"\"\"Is mass balance being confirmed in the study? Mass Balance involves accounting for the total amount of a drug that enters and exits a biological system. It is sometimes abbreviated as MB. \"\"\"\n",
        ",\"\"\"Is Food effect mentioned in this text? Food effect describes how the presence or absence of food in the stomach can affect the rate and extent to which a drug is absorbed into the bloodstream. It is sometimes abbreviated as FE. \"\"\"\n",
        ",\"\"\"Patients, subjects, and participants are used interchangeably. Other synonyms are Enrollees, study volunteers, research recruits, cohort members, survey respondents. How many patients are planned for study according to the text?  \"\"\"\n",
        ",\"\"\"What is the age of participants mentioned in the text? \"\"\"\n",
        ",\"\"\"Are adult participants a part of this trial? \"\"\"\n",
        ",\"\"\"Is it a study checking for influence of the therapy on subjects of a specific ethnicity? \"\"\"\n",
        ",\"\"\"Is competing trial mentioned in the text? Competing trial investigates a similar or related intervention as the trial in question. \"\"\"\n",
        ",\"\"\"Is it mentioned in the text that the target population is uncommon? \"\"\"\n",
        ",\"\"\"Is the “target is common” mentioned in the text? \"\"\"\n",
        ",\"\"\"Is it a study related to orphan diseases mentioned in the text? Orphan diseases are rare diseases that affect a relatively small number of individuals. \"\"\"\n",
        ",\"\"\"Does the study involve any disease which has a limited number of medications available in the market to treat it? \"\"\"\n",
        ",\"\"\"Is paediatric population involved in the study? They are sometimes referred to as children or adolescents. \"\"\"\n",
        ",\"\"\"Is molecular screening a criterion to select patients in the study? Molecular screening criteria refers to the specific genetic, molecular, or biomarker-based characteristics used to identify and select patients for participation in a clinical trial. \"\"\"\n",
        ",\"\"\"Does the study involve inclusion criteria of highly selective eligibility?  \"\"\"\n",
        ",\"\"\"Is it an open label enrolment study? It is a study where participants are aware of the treatment they are receiving and can enroll themselves directly into a particular treatment group or study arm without randomization by researchers. \"\"\"\n",
        ",\"\"\"Is it a study with no randomization during enrolment? \"\"\"\n",
        ",\"\"\"Is it an open label randomization study? It is a study where both the researchers and the participants are aware of the treatment they are receiving. The randomization process is still used to assign participants to different treatment groups, but everyone involved knows which treatment or intervention the participant is receiving. \"\"\"\n",
        ",\"\"\"Is it a double blind randomization clinical study? \"\"\"\n",
        ",\"\"\"Oral dosing refers to the administration of drugs through the mouth usually swallowed. Does the study involve oral dosing? \"\"\"\n",
        ",\"\"\"Is multiple drug formulation mentioned in the text of dosing section? Multiple drug formulation means a medicinal product being administered in multiple forms. For example, a drug being presented as a powder in capsule as well as a liquid-filled capsule.  \"\"\"\n",
        ",\"\"\"Is rescue medication allowed to be used as mentioned in the text of dosing section? Rescue medications are medications that help in managing conditions that involve sudden symptom exacerbations by providing quick relief. Common rescue medications are Epipen, adrenaline, steroids, triptans, antihistamines etc. \"\"\"\n",
        ",\"\"\"What is the treatment period mentioned in the study? \"\"\"\n",
        ",\"\"\"In an inpatient study, participants are admitted to a study site or are admitted to a clinic. In an inpatient study, the text also might mention subjects checking in and getting discharged. Is it an inpatient study? \"\"\"\n",
        ",\"\"\"In an outpatient study, participants visit the study site or must visit a clinic or must visit a hospital. In an outpatient study, participants do not stay overnight. Is it an outpatient study?   \"\"\"\n",
        ",\"\"\"Are the lab samples sent to one central lab? Lab samples are sometimes referred to as biomarkers, FBR, Fasting Blood referrals or bodily fluid samples. \"\"\"\n",
        ",\"\"\"Are the lab samples sent to a different location other than a central lab? Lab samples are sometimes referred to as biomarkers, FBR, Fasting Blood referrals or bodily fluid samples. \"\"\"\n",
        ",\"\"\"Do we require complex frozen packaging for shipment of the lab samples? Complex frozen packaging can involve dry ice, liquid nitrogen, refrigeration, freezer boxes, vacuum insulated dry shipper containers, thermoformed packaging, insulated shipping kits or any kind of customized packagings. \"\"\"\n",
        ",\"\"\"Is Cohort Safety Review mentioned in the text? \"\"\"\n",
        ",\"\"\"Is Dose Escalation Review Team a part of the study? They are sometimes referred to as DERT. \"\"\"\n",
        ",\"\"\"Is Dose Review Committee a part of the study? They are sometimes referred to as DRC. \"\"\"\n",
        ",\"\"\"Is the data collection happening at the site for this study? \"\"\"\n",
        ",\"\"\"Are Adverse Events of Special Interest mentioned and applicable for the study? They are sometimes abbreviated as AESI or AEs of Special Interest. \"\"\"\n",
        ",\"\"\"Alanine aminotransferase is sometimes abbreviated as ALT. Aspartate aminotransferase is sometimes abbreviated as AST. Upper Limit of Normal is sometimes abbreviated as ULN. Is elevation of alanine aminotransferase or aspartate aminotransferase being compared with Upper limit of normal in the study? \"\"\"\n",
        ",\"\"\"Upper Limit of Normal is sometimes abbreviated as ULN. Is total bilirubin level being compared with upper limit of normal in the study? \"\"\"\n",
        ",\"\"\"Does any section name in the pdf contain Interim as a substring?  \"\"\"\n",
        ",\"\"\"Are there section names mentioning IA? \"\"\"\n",
        ",\"\"\"Are there any mentions of snapshots in the text? \"\"\"\n",
        ",\"\"\"Is there unblinding prior to full database lock? \"\"\"\n",
        ",\"\"\"How many sites are planned for the study? \"\"\"\n",
        ",\"\"\"How many countries are planned for the study? \"\"\"\n",
        ",\"\"\"Is a Non-USA country involved in the study? \"\"\"\n",
        ",\"\"\"How many protocol amendments were made according to the text? \"\"\"\n",
        ",\"\"\"Are there any country specific amendments made to the protocol? \"\"\"\n",
        ",\"\"\"Does the study involve Clinical Outcome Assessments? They are sometimes referred to as COA, ClinRO or Clinician Reported Outcomes. \"\"\"\n",
        ",\"\"\"Does the study involve Patient Reported Outcomes? They are sometimes referred to as PRO. \"\"\"\n",
        ",\"\"\"Case Report Form is sometimes abbreviated as CRF. Does the study mention that some data needs to be transcribed or entered in a CRF?  \"\"\"\n",
        ",\"\"\"Are any of these terms mentioned in the text: adhd-rs-5, adpkd outcome, conners-3, ids-sr, msfq, pgi-c, pgi-s, qol questionnaire, sds, smwq-p, smwq-po, Diary, EDE-Q, SF-36? \"\"\"\n",
        ",\"\"\"Does the study involve electronic Clinical Outcome Assessments? They are sometimes referred to as eCOA, eClinRO or electronic Clinician Reported Outcomes. \"\"\"\n",
        ",\"\"\"Does the study involve electronic Patient Reported Outcomes? They are sometimes referred to as ePRO. \"\"\"\n",
        ",\"\"\"Does the study involve self-reporting? \"\"\"\n",
        ",\"\"\"Does the study involve observer-reporting? \"\"\"\n",
        ",\"\"\"Does the study involve caregiver reporting?  \"\"\"\n",
        ",\"\"\"Does the study mention performance outcomes? \"\"\"\n",
        ",\"\"\"Does the study involve arater? \"\"\"\n",
        ",\"\"\"Does the study involve a certified clinician? \"\"\"\n",
        ",\"\"\"Is it mentioned in the text that any of these assessments would be performed electronically: adhd-rs-5, adpkd outcome, conners-3, ids-sr, msfq, pgi-c, pgi-s, qol questionnaire, sds, smwq-p, smwq-po, Diary, EDE-Q, SF-36? \"\"\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4dTM2I7sMky"
      },
      "source": [
        "### Groundedness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIJCmWyGEQgO",
        "outputId": "7375571a-d38e-4215-f580-a15b5c90b226"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select_ground_truth(\"What is the treatment period mentioned in the study?\", questions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# response = sentence_pipeline.query(\"What is the treatment period mentioned in the study?\")\n",
        "# print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3d-fKfLsOPy"
      },
      "source": [
        "### QS Relevance With CoT reasons (Open Source Implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ce-k6p1Zppe_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8f87a755e0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8f87a755e0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8f87a755e0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function groundedness_measure_with_cot_reasons at 0x7f8f87a755e0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
            "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/venv/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"number\" with the value 1. The structure is identical, so all values match.\n",
            "SCORE: 10\n",
            "(1.0, {'reason': ': Both JSON objects have a key \"number\" with the value 1. The structure is identical, so all values match.\\n'})\n"
          ]
        }
      ],
      "source": [
        "prompt = \"How many total number of drugs are being assessed as Investigational Medicinal Product in this study?\"\n",
        "response = automerging_pipeline.query(prompt)\n",
        "relevance = gt_relevance_with_cot_reasons(prompt, response)\n",
        "print(relevance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEwcAjy5w7X"
      },
      "source": [
        "### Initialize all Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3kTTsglpSkn",
        "outputId": "afbf82d6-fe93-4597-a0ef-bf22baa6779b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a754c0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a754c0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function gt_relevance_with_cot_reasons at 0x7f8f87a754c0> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n",
            "Feedback implementation <function relevance_with_cot_reasons at 0x7f8f87a75550> cannot be serialized: Module __main__ is not importable.. This may be ok unless you are using the deferred feedback mode.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Ground-Truth Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Ground-Truth Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jx_H8Z56sr5"
      },
      "source": [
        "## Setup Eval Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oq4vc1mKpShz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
            "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "del sentence_pipeline, automerging_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF-TThDkpSfT",
        "outputId": "7818a533-0611-4b3a-9d69-a5387e5f7d86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function BaseQueryEngine.query at 0x7f9c1a5a1b80>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n",
            "A new object of type <class 'llama_index.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> at 0x7f9c04705dc0 is calling an instrumented method <function BaseRetriever.retrieve at 0x7f9c1a59adc0>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app.retriever based on other object (0x7f9c1979e6d0) using this function.\n",
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function CompactAndRefine.get_response at 0x7f9c1918daf0>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "A new object of type <class 'llama_index.llm_predictor.base.LLMPredictor'> at 0x7f9c32e43f00 is calling an instrumented method <function LLMPredictor.predict at 0x7f9c220f4b80>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer.service_context.llm_predictor based on other object (0x7f9c045d4480) using this function.\n",
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 1/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 2/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n",
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the exact same key \"presence\" with the same value \"False\". Therefore, they represent identical data.\n",
            "SCORE: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 3/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 4/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has both \"number\" and \"healthy_subjects\", while the second one only contains \"number\". There is no comparison possible since they have different structures.\n",
            "SCORE: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 5/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has keys'min_number' and'max_number', while the second one has different keys like 'age_range', 'page_no', and 'protocol_mention'. There is no direct comparison possible between these sets of data.\n",
            "SCORE: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 6/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has a key \"presence\" with value 'None', while the second one has both \"presence\" and another key \"applicability\". Since they have different keys and values, their presence is not the same.\n",
            "SCORE: 0\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the key \"presence\" with the value set to False. Since all the relevant information is identical, we can conclude that they represent the same data.\n",
            "SCORE: 10\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"presence\" with the value set to True. The structure and values are identical.\n",
            "SCORE: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 7/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 8/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n",
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 9/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the exact same key \"presence\" with the same value \"False\". Therefore, they represent the same data.\n",
            "SCORE: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has a key \"presence\" with value False while the second one has a different key \"number\" with value 1. There is no matching key-value pair between these two objects.\n",
            "SCORE: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 10/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 11/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"presence\" with the value set to True. The structure and values are identical.\n",
            "SCORE: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 12/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 13/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n",
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has \"placebo_controlled\" as a key while the second one has \"placebo-controlled\". Both have the same value (True). However, there is no matching key for \"open\", \"randomized\", \"double-blind\", and \"unknown\" in the first JSON. These additional keys make both JSONs different from each other.\n",
            "\n",
            "SCORE: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7f9c04705d00 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7f9c2cb34e50>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app based on other object (0x7f9c1979e5e0) using this function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ran Question 14/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7f9c04705fd0 is calling an instrumented method <function Refine.get_response at 0x7f9c1918d700>. The path of this call may be incorrect.\n",
            "Guessing path of new object is app._response_synthesizer based on other object (0x7f9c1979e070) using this function.\n",
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has three key-value pairs while the second one has just one. There is no comparison possible as they have different structures and keys.\n",
            "SCORE: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has a key \"presence\" with value False while the second one has a different key \"number\" with value 1. There is no matching key-value pair between these two JSONs.\n",
            "SCORE: 0\n",
            "Ran Question 15/15 for Sentence Window RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has additional key-value pairs like 'gender' which is missing from the second one. Also, there are new keys present in the second JSON such as 'total_subjects', 'arm_1_subjects', and 'arm_2_subjects'. These differences make them dissimilar.\n",
            "SCORE: 0\n",
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has more information like dose_administration, IMP_administration, while the second one is missing those details. Only presence is common to both.\n",
            "SCORE: 3\n",
            "Ran Question 1/15 for Auto-Merging RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The value \"presence\" is different in both objects as it's either True or False.\n",
            "SCORE: 0\n",
            "Ran Question 2/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the key \"presence\" with a value of False. The structure and content of both objects are identical.\n",
            "SCORE: 10\n",
            "Ran Question 3/15 for Auto-Merging RAG\n",
            "Ran Question 4/15 for Auto-Merging RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has a key \"presence\" with value False while the second one has a different key \"number\" with value 88. There is no commonality between these two JSONs.\n",
            "SCORE: 0\n",
            "Ran Question 5/15 for Auto-Merging RAG\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has a key \"presence\" with value False while the second one also has a key \"presence\" but its value is True. There's no other matching key-value pair. So, they don't have the same presence value.\n",
            "SCORE: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feedback Function exception caught: Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/venv/lib/python3.8/site-packages/trulens_eval/feedback/feedback.py\", line 519, in run\n",
            "    assert isinstance(\n",
            "AssertionError: Feedback function output must be a float or dict but was <class 'int'>.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has keys'min_number' and'max_number', while the second one has different keys like 'age_range', 'page_number', and 'protocol_reference'. Although both have numbers related to age range (18-55), they don't share the exact same structure.\n",
            "SCORE: 0\n",
            "Ran Question 6/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the exact same key \"presence\" with the same value \"False\". Therefore, they represent identical data.\n",
            "SCORE: 10\n",
            "Ran Question 7/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"presence\" with the value set to True. The structure and content of both dictionaries are identical.\n",
            "SCORE: 10\n",
            "Ran Question 8/15 for Auto-Merging RAG\n",
            "Ran Question 9/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have the exact same key \"presence\" with the same value \"False\". Therefore, they represent identical data.\n",
            "SCORE: 10\n",
            "Ran Question 10/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"number\" with the value 1. The structure is identical, so we can conclude that they represent the same data.\n",
            "SCORE: 10\n",
            "Ran Question 11/15 for Auto-Merging RAG\n",
            "Ran Question 12/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"presence\" with the value set to True. The structure and values are identical.\n",
            "SCORE: 10\n",
            "Ran Question 13/15 for Auto-Merging RAG\n",
            "Ran Question 14/15 for Auto-Merging RAG\n",
            "Response: \n",
            "  \n",
            " REASONS: The first JSON has fewer key-value pairs compared to the second one. While both have \"placebo-controlled\", \"open\", and their respective values as True and False, respectively, they also contain additional keys like \"randomized\" and \"double-blind\" which are missing from the first JSON string.\n",
            "\n",
            "SCORE: 7 (Since most of the important information is present in both JSONs but there are some extra details in the second JSON that aren't found in the first.)\n",
            "Response: \n",
            "  \n",
            " REASONS: Both JSON objects have a key \"number\" with the value 1. The structure is identical, so all the values match.\n",
            "SCORE: 10\n",
            "Ran Question 15/15 for Auto-Merging RAG\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>app_json</th>\n",
              "      <th>type</th>\n",
              "      <th>record_id</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>tags</th>\n",
              "      <th>record_json</th>\n",
              "      <th>cost_json</th>\n",
              "      <th>perf_json</th>\n",
              "      <th>...</th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>Ground-Truth Relevance_calls</th>\n",
              "      <th>Answer Relevance_calls</th>\n",
              "      <th>Context Relevance_calls</th>\n",
              "      <th>Groundedness_calls</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_tokens</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence Window RAG</td>\n",
              "      <td>{\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_e5f466c8fd8b34ba1b24ee83e833054a</td>\n",
              "      <td>\"Does this trial involve Psychedelic Research,...</td>\n",
              "      <td>\"The context information does not specifically...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_e5f466c8fd8b34ba1b2...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-13T08:14:35.409204\", \"...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.65</td>\n",
              "      <td>[{'args': {'prompt': 'Does this trial involve ...</td>\n",
              "      <td>[{'args': {'prompt': 'Does this trial involve ...</td>\n",
              "      <td>[{'args': {'prompt': 'Does this trial involve ...</td>\n",
              "      <td>[{'args': {'source': 'Assessments for safety: ...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence Window RAG</td>\n",
              "      <td>{\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_075dcee660ec0eeba2e36c0a7d87142d</td>\n",
              "      <td>\"Patients, subjects, and participants are used...</td>\n",
              "      <td>\"According to the text, approximately 88 healt...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_075dcee660ec0eeba2e...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-13T08:14:38.202246\", \"...</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.00</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'args': {'prompt': 'Patients, subjects, and ...</td>\n",
              "      <td>[{'args': {'prompt': 'Patients, subjects, and ...</td>\n",
              "      <td>[{'args': {'source': 'The principal investigat...</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence Window RAG</td>\n",
              "      <td>{\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_500c3a4781e62e6b15a78fd1998187d6</td>\n",
              "      <td>\"What is the age of participants mentioned in ...</td>\n",
              "      <td>\"The age of participants mentioned in the text...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_500c3a4781e62e6b15a...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-13T08:15:04.941123\", \"...</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.85</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'args': {'prompt': 'What is the age of parti...</td>\n",
              "      <td>[{'args': {'prompt': 'What is the age of parti...</td>\n",
              "      <td>[{'args': {'source': 'Subjects will not be all...</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence Window RAG</td>\n",
              "      <td>{\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_6b639bb1e3d5450f7a7a2663dc912408</td>\n",
              "      <td>\"Are Adverse Events of Special Interest mentio...</td>\n",
              "      <td>\"Adverse Events of Special Interest (AESIs) ar...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_6b639bb1e3d5450f7a7...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-13T08:15:30.428140\", \"...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'args': {'prompt': 'Are Adverse Events of Sp...</td>\n",
              "      <td>[{'args': {'prompt': 'Are Adverse Events of Sp...</td>\n",
              "      <td>[{'args': {'source': '• Other medically signif...</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence Window RAG</td>\n",
              "      <td>{\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...</td>\n",
              "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
              "      <td>record_hash_fbd0bb4d286dbea375431e294eb09810</td>\n",
              "      <td>\"What is the treatment period mentioned in the...</td>\n",
              "      <td>\"The treatment period mentioned in the study i...</td>\n",
              "      <td>-</td>\n",
              "      <td>{\"record_id\": \"record_hash_fbd0bb4d286dbea3754...</td>\n",
              "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
              "      <td>{\"start_time\": \"2023-12-13T08:16:55.200876\", \"...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.90</td>\n",
              "      <td>[{'args': {'prompt': 'What is the treatment pe...</td>\n",
              "      <td>[{'args': {'prompt': 'What is the treatment pe...</td>\n",
              "      <td>[{'args': {'prompt': 'What is the treatment pe...</td>\n",
              "      <td>[{'args': {'source': '4.4 End of Trial Definit...</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                app_id                                           app_json  \\\n",
              "0  Sentence Window RAG  {\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...   \n",
              "1  Sentence Window RAG  {\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...   \n",
              "2  Sentence Window RAG  {\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...   \n",
              "3  Sentence Window RAG  {\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...   \n",
              "4  Sentence Window RAG  {\"app_id\": \"Sentence Window RAG\", \"tags\": \"-\",...   \n",
              "\n",
              "                                                type  \\\n",
              "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
              "\n",
              "                                      record_id  \\\n",
              "0  record_hash_e5f466c8fd8b34ba1b24ee83e833054a   \n",
              "1  record_hash_075dcee660ec0eeba2e36c0a7d87142d   \n",
              "2  record_hash_500c3a4781e62e6b15a78fd1998187d6   \n",
              "3  record_hash_6b639bb1e3d5450f7a7a2663dc912408   \n",
              "4  record_hash_fbd0bb4d286dbea375431e294eb09810   \n",
              "\n",
              "                                               input  \\\n",
              "0  \"Does this trial involve Psychedelic Research,...   \n",
              "1  \"Patients, subjects, and participants are used...   \n",
              "2  \"What is the age of participants mentioned in ...   \n",
              "3  \"Are Adverse Events of Special Interest mentio...   \n",
              "4  \"What is the treatment period mentioned in the...   \n",
              "\n",
              "                                              output tags  \\\n",
              "0  \"The context information does not specifically...    -   \n",
              "1  \"According to the text, approximately 88 healt...    -   \n",
              "2  \"The age of participants mentioned in the text...    -   \n",
              "3  \"Adverse Events of Special Interest (AESIs) ar...    -   \n",
              "4  \"The treatment period mentioned in the study i...    -   \n",
              "\n",
              "                                         record_json  \\\n",
              "0  {\"record_id\": \"record_hash_e5f466c8fd8b34ba1b2...   \n",
              "1  {\"record_id\": \"record_hash_075dcee660ec0eeba2e...   \n",
              "2  {\"record_id\": \"record_hash_500c3a4781e62e6b15a...   \n",
              "3  {\"record_id\": \"record_hash_6b639bb1e3d5450f7a7...   \n",
              "4  {\"record_id\": \"record_hash_fbd0bb4d286dbea3754...   \n",
              "\n",
              "                                           cost_json  \\\n",
              "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "2  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "3  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "4  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
              "\n",
              "                                           perf_json  ... Answer Relevance  \\\n",
              "0  {\"start_time\": \"2023-12-13T08:14:35.409204\", \"...  ...              0.4   \n",
              "1  {\"start_time\": \"2023-12-13T08:14:38.202246\", \"...  ...              1.0   \n",
              "2  {\"start_time\": \"2023-12-13T08:15:04.941123\", \"...  ...              1.0   \n",
              "3  {\"start_time\": \"2023-12-13T08:15:30.428140\", \"...  ...              0.6   \n",
              "4  {\"start_time\": \"2023-12-13T08:16:55.200876\", \"...  ...              0.9   \n",
              "\n",
              "   Context Relevance  Groundedness  \\\n",
              "0                0.2          0.65   \n",
              "1                0.9          1.00   \n",
              "2                1.0          0.85   \n",
              "3                1.0          0.75   \n",
              "4                0.9          0.90   \n",
              "\n",
              "                        Ground-Truth Relevance_calls  \\\n",
              "0  [{'args': {'prompt': 'Does this trial involve ...   \n",
              "1                                                 []   \n",
              "2                                                 []   \n",
              "3                                                 []   \n",
              "4  [{'args': {'prompt': 'What is the treatment pe...   \n",
              "\n",
              "                              Answer Relevance_calls  \\\n",
              "0  [{'args': {'prompt': 'Does this trial involve ...   \n",
              "1  [{'args': {'prompt': 'Patients, subjects, and ...   \n",
              "2  [{'args': {'prompt': 'What is the age of parti...   \n",
              "3  [{'args': {'prompt': 'Are Adverse Events of Sp...   \n",
              "4  [{'args': {'prompt': 'What is the treatment pe...   \n",
              "\n",
              "                             Context Relevance_calls  \\\n",
              "0  [{'args': {'prompt': 'Does this trial involve ...   \n",
              "1  [{'args': {'prompt': 'Patients, subjects, and ...   \n",
              "2  [{'args': {'prompt': 'What is the age of parti...   \n",
              "3  [{'args': {'prompt': 'Are Adverse Events of Sp...   \n",
              "4  [{'args': {'prompt': 'What is the treatment pe...   \n",
              "\n",
              "                                  Groundedness_calls latency total_tokens  \\\n",
              "0  [{'args': {'source': 'Assessments for safety: ...       2            0   \n",
              "1  [{'args': {'source': 'The principal investigat...      26            0   \n",
              "2  [{'args': {'source': 'Subjects will not be all...      25            0   \n",
              "3  [{'args': {'source': '• Other medically signif...      84            0   \n",
              "4  [{'args': {'source': '4.4 End of Trial Definit...      65            0   \n",
              "\n",
              "   total_cost  \n",
              "0         0.0  \n",
              "1         0.0  \n",
              "2         0.0  \n",
              "3         0.0  \n",
              "4         0.0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions = [x['question'] for x in questions_list]\n",
        "\n",
        "tot=len(questions)\n",
        "with sentence_recorder as recording:\n",
        "    for i,question in enumerate(questions):\n",
        "        response = sentence_pipeline.query(question)\n",
        "        print(f\"Ran Question {i+1}/{tot} for Sentence Window RAG\")\n",
        "\n",
        "\n",
        "with automerging_recorder as recording:\n",
        "    for i,question in enumerate(questions):\n",
        "        response = automerging_pipeline.query(question)\n",
        "        print(f\"Ran Question {i+1}/{tot} for Auto-Merging RAG\")\n",
        "\n",
        "\n",
        "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
        "records.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "1qZI3dUSEos-",
        "outputId": "d0cab92a-5072-49c7-b47d-dc29e4c61d81"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ground-Truth Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Auto-Merging Window RAG</th>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.783333</td>\n",
              "      <td>38.666667</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentence Window RAG</th>\n",
              "      <td>0.709091</td>\n",
              "      <td>0.481818</td>\n",
              "      <td>0.677273</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>29.545455</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Ground-Truth Relevance  Context Relevance  \\\n",
              "app_id                                                               \n",
              "Auto-Merging Window RAG                0.820000           0.458333   \n",
              "Sentence Window RAG                    0.709091           0.481818   \n",
              "\n",
              "                         Groundedness  Answer Relevance    latency  total_cost  \n",
              "app_id                                                                          \n",
              "Auto-Merging Window RAG      0.700000          0.783333  38.666667         0.0  \n",
              "Sentence Window RAG          0.677273          0.545455  29.545455         0.0  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from trulens_eval import Tru, TruLlama\n",
        "tru = Tru()\n",
        "tru.get_leaderboard(app_ids=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Force stopping dashboard ...\n"
          ]
        }
      ],
      "source": [
        "tru.stop_dashboard(force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvAZTRyqCCUU",
        "outputId": "b9453700-7e4d-4a79-ca6e-8371c201cb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dashboard ...\n",
            "Config file already exists. Skipping writing process.\n",
            "Credentials file already exists. Skipping writing process.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42bfd880b231467095beeb18d09e23c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dashboard started at http://172.31.28.48:8501 .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<subprocess.Popen at 0x7f270632e4f0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/venv/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "  \n",
            " REASONS: The provided statement gives clear details on the number of subjects per arm and their respective cohort sizes, allowing us to calculate the total number of subjects involved in the study.\n",
            "\n",
            "SCORE: 9\n",
            "Response: \n",
            "  \n",
            " REASONS: This answer provides a clear explanation of how many dose cohorts are planned for each arm (Arm 1 has 7, Arm 2 has either 3 or 4) and gives a summary of the overall number of cohorts (at least 11). It directly addresses the information asked about the study's arms and their respective cohort plans.\n",
            "\n",
            "SCORE: 9\n",
            "Response: \n",
            "  \n",
            " REASONS: Both answers provide similar information about the age range of participants being from 18 to 55 years old. However, the correct answer includes \"of age\" which makes it clear that the ages are included within the range (18-55), while the provided answer does not explicitly mention inclusion or exclusion. Additionally, the correct answer also mentions where the information was found (Page 33 of the protocol). These details make the correct answer more precise and informative.\n",
            "\n",
            "SCORE: 9\n"
          ]
        }
      ],
      "source": [
        "# tru.run_dashboard()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNNGMY7dq57xWDBZjuE2Cl9",
      "collapsed_sections": [
        "bpw9cfg2415x",
        "q0xq43o56XIc",
        "fehUYyDm6Aex"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
