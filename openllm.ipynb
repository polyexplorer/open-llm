{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"openllm[gptq]\" --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/open-llm/openllm.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-233-246-109.ap-south-1.compute.amazonaws.com/home/ubuntu/open-llm/openllm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenllm\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-233-246-109.ap-south-1.compute.amazonaws.com/home/ubuntu/open-llm/openllm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m client \u001b[39m=\u001b[39m openllm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mHTTPClient(\u001b[39m'\u001b[39m\u001b[39mhttp://localhost:3000\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-233-246-109.ap-south-1.compute.amazonaws.com/home/ubuntu/open-llm/openllm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m client\u001b[39m.\u001b[39;49mquery(\u001b[39m'\u001b[39;49m\u001b[39mExplain to me the difference between \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfurther\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m and \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfarther\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/_base.py:268\u001b[0m, in \u001b[0;36mBaseClient.query\u001b[0;34m(self, prompt, return_response, **attrs)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[39mif\u001b[39;00m return_attrs \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m: return_response \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mattrs\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    267\u001b[0m use_default_prompt_template \u001b[39m=\u001b[39m attrs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39muse_default_prompt_template\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 268\u001b[0m prompt, generate_kwargs, postprocess_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39msanitize_parameters(prompt, use_default_prompt_template\u001b[39m=\u001b[39muse_default_prompt_template, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs)\n\u001b[1;32m    269\u001b[0m r \u001b[39m=\u001b[39m openllm_core\u001b[39m.\u001b[39mGenerationOutput(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(\u001b[39m'\u001b[39m\u001b[39mgenerate\u001b[39m\u001b[39m'\u001b[39m, openllm_core\u001b[39m.\u001b[39mGenerationInput(prompt\u001b[39m=\u001b[39mprompt, llm_config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel_construct_env(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs))\u001b[39m.\u001b[39mmodel_dump()))\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m return_response \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mattrs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/_base.py:154\u001b[0m, in \u001b[0;36m_ClientAttr.config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfig\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m openllm_core\u001b[39m.\u001b[39mLLMConfig:\n\u001b[0;32m--> 154\u001b[0m   \u001b[39mreturn\u001b[39;00m openllm_core\u001b[39m.\u001b[39mAutoConfig\u001b[39m.\u001b[39mfor_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name)\u001b[39m.\u001b[39mmodel_construct_env(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfiguration)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/_base.py:106\u001b[0m, in \u001b[0;36m_ClientAttr.model_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_name\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    105\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metadata[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    107\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mMalformed service endpoint. (Possible malicious)\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/_base.py:101\u001b[0m, in \u001b[0;36m_ClientAttr._metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_metadata\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 101\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m'\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/_base.py:165\u001b[0m, in \u001b[0;36m_Client.call\u001b[0;34m(self, api_name, *args, **attrs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, api_name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 165\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner\u001b[39m.\u001b[39;49mcall(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mapi_name\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_api_version\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mattrs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/benmin/__init__.py:43\u001b[0m, in \u001b[0;36mClient.call\u001b[0;34m(self, bentoml_api_name, data, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, bentoml_api_name: \u001b[39mstr\u001b[39m, data: t\u001b[39m.\u001b[39mAny \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m---> 43\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(data, _inference_api\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msvc\u001b[39m.\u001b[39;49mapis[bentoml_api_name], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_client/benmin/_http.py:94\u001b[0m, in \u001b[0;36mHttpClient._call\u001b[0;34m(self, data, _inference_api, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m   fake_resp \u001b[39m=\u001b[39m ensure_exec_coro(_inference_api\u001b[39m.\u001b[39minput\u001b[39m.\u001b[39mto_http_response(kwargs, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m   fake_resp \u001b[39m=\u001b[39m ensure_exec_coro(_inference_api\u001b[39m.\u001b[39;49minput\u001b[39m.\u001b[39;49mto_http_response(data, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m     96\u001b[0m \u001b[39m# XXX: hack around StreamingResponse, since now we only have Text, for metadata so it is fine to do this.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fake_resp, starlette\u001b[39m.\u001b[39mresponses\u001b[39m.\u001b[39mStreamingResponse): body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/openllm_core/utils/__init__.py:74\u001b[0m, in \u001b[0;36mensure_exec_coro\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mensure_exec_coro\u001b[39m(coro: t\u001b[39m.\u001b[39mCoroutine[t\u001b[39m.\u001b[39mAny, t\u001b[39m.\u001b[39mAny, t\u001b[39m.\u001b[39mAny]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m     73\u001b[0m   loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_event_loop()\n\u001b[0;32m---> 74\u001b[0m   \u001b[39mif\u001b[39;00m loop\u001b[39m.\u001b[39mis_running(): \u001b[39mreturn\u001b[39;00m asyncio\u001b[39m.\u001b[39;49mrun_coroutine_threadsafe(coro, loop)\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m     75\u001b[0m   \u001b[39melse\u001b[39;00m: \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39mrun_until_complete(coro)\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import openllm\n",
    "client = openllm.client.HTTPClient('http://localhost:3000')\n",
    "client.query('Explain to me the difference between \"further\" and \"farther\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"UTF-8\">\\n    <title>BentoML Prediction Service</title>\\n    <!-- Google Tag Manager -->\\n    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\n    new Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\n    j=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\n    \\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n    })(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-WNPGWRM\\');</script>\\n    <!-- End Google Tag Manager -->\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"./static_content/swagger-ui.css\" />\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"./static_content/index.css\" />\\n    <link rel=\"icon\" type=\"image/png\" href=\"./static_content/favicon-light-32x32.png\" sizes=\"32x32\" media=\"(prefers-color-scheme: light)\" />\\n    <link rel=\"icon\" type=\"image/png\" href=\"./static_content/favicon-dark-32x32.png\" sizes=\"32x32\" media=\"(prefers-color-scheme: dark)\" />\\n  </head>\\n  <body>\\n    <!-- Google Tag Manager (noscript) -->\\n    <noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-WNPGWRM\"\\n    height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\\n    <!-- End Google Tag Manager (noscript) -->\\n    <div id=\"swagger-ui\"></div>\\n    <script src=\"./static_content/swagger-ui-bundle.js\" charset=\"UTF-8\"> </script>\\n    <script src=\"./static_content/swagger-ui-standalone-preset.js\" charset=\"UTF-8\"> </script>\\n    <script src=\"./static_content/swagger-initializer.js\" charset=\"UTF-8\"> </script>\\n    <div class=\"version\">\\n        <div class=\"version-section\"><a href=\"https://github.com/bentoml/BentoML\" class=\"github-corner\" aria-label=\"Powered by BentoML\">\\n            <svg width=\"80\" height=\"80\" viewBox=\"0 0 250 250\" style=\"fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;\" aria-hidden=\"true\">\\n            <path d=\"M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z\"></path>\\n            <path d=\"M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2\" fill=\"currentColor\" style=\"transform-origin: 130px 106px;\" class=\"octo-arm\"></path>\\n            <path d=\"M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z\" fill=\"currentColor\" class=\"octo-body\"></path>\\n            </svg></a>\\n        </div>\\n    </div>\\n  </body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(str(requests.get(\"http://localhost:3000/\")._content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:3000/v1/metadata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
