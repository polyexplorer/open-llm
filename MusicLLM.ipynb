{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3a2ad2-e093-4341-9724-0bc95aa9db14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Add venv to kernel\n",
    "# ! python3 -m venv venv\n",
    "# ! source venv/bin/activate\n",
    "# ! pip install jupyter\n",
    "# ! ipython kernel install --name \"venv\" --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bda546e-dfd6-4159-a10f-60e45f33c223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Dependencies \n",
    "# ! pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
    "# ! pip install optimum\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
    "# ! pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n",
    "# ! pip install langchain\n",
    "# ! pip install langchainhub\n",
    "# ! pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5934ea3a-aa32-4ebe-8232-02aad3257580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mistral Wrapper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,GPTQConfig, pipeline,TextStreamer\n",
    "import torch\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class MistralModel:\n",
    "    def __init__(self):\n",
    "        # Refresh CUDA Memory\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model,self.tokenizer = self.get_model()\n",
    "        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def format_prompt(self,prompt):\n",
    "        return f\"\"\"<s>[INST] {prompt} [/INST]\"\"\"\n",
    "    \n",
    "    def generate_instruction(\n",
    "        self,\n",
    "        prompt:str,\n",
    "        instruction:str = 'Think carefully and answer the given question as truthfully as possible',\n",
    "        llm_template = None\n",
    "    ):\n",
    "        if not llm_template:\n",
    "            llm_template = self.format_prompt\n",
    "        instruction_format = f\"\"\"### Instruction: {instruction}:\n",
    "\n",
    "    ### Input:\n",
    "    Question: {prompt}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "        if llm_template:\n",
    "            return llm_template(instruction_format)\n",
    "        else:\n",
    "            return instruction_format\n",
    "\n",
    "    \n",
    "    def get_model(self):\n",
    "        model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "        # To use a different branch, change revision\n",
    "        # For example: revision=\"main\"\n",
    "        quantization_config_loading = GPTQConfig(bits=4, use_exllama = False)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "\n",
    "                                                  quantization_config=quantization_config_loading,\n",
    "                                                  device_map=\"cuda\",\n",
    "                                                  trust_remote_code=True,\n",
    "                                                  revision=\"gptq-4bit-32g-actorder_True\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def _predict(self, prompt):\n",
    "        return self.pipe(self.format_prompt(prompt))[0]['generated_text']\n",
    "    \n",
    "    def predict(self,prompt):\n",
    "        return self._predict(self.format_prompt(prompt)).split(r'INST]')[-1].strip()\n",
    "    \n",
    "    def ask(self,question,instruction = None):\n",
    "        formatted_prompt = self.generate_instruction(prompt=question,instruction=instruction)\n",
    "        return self.predict(formatted_prompt)\n",
    "    \n",
    "class MistralLLM(LLM):\n",
    "    mistral_model: MistralModel\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        # if stop is not None:\n",
    "        #     raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return self.mistral_model.ask(prompt)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model\": self.mistral_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cb9a75-4b70-43e3-9fcb-3ec6c01264aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# if model:\n",
    "#     del model\n",
    "model = MistralModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd52af5-8a12-420d-a357-960e5206928d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Answer 4\n"
     ]
    }
   ],
   "source": [
    "answer = model.predict(\"What is 2+2?\")\n",
    "print(\"Answer\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b08f60ee-135d-421c-84d1-1fe130eb8f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ed914ce-d754-4461-8224-a9a221b26c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = MistralLLM(mistral_model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db633e88-3d21-4ce1-bd65-3480b9d9cf48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralLLM(mistral_model=<__main__.MistralModel object at 0x7f7164ea9390>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01d4eb5b-4183-4698-a7a0-43a19e9334ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Self Ask Engine\n",
    "\n",
    "def generate_qa_pairs(\n",
    "    mistral_model: MistralLLM,\n",
    "    corpus: str,\n",
    "    instruction: str = \"You are a good generator of question answer pairs. Given the text, generate 5 question and answer pairs.\"\n",
    "): \n",
    "    raw_qa = mistral_model.ask(corpus,instruction=instruction)\n",
    "    # return raw_qa\n",
    "    lines = raw_qa.split('\\n')\n",
    "    questions_list = lines[::2]\n",
    "    answers_list = lines[1::2]\n",
    "    qa_list = [ {'question':q[3:],'answer':a.replace('Answer:','').strip()} for q,a in zip(questions_list,answers_list)]\n",
    "    return qa_list\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f612c6ca-c1a5-414f-8357-7c8a07efad03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"You are a good generator of question answer pairs. Given the text, generate question and answer pairs.\"\n",
    "# instruction = None\n",
    "question = \"\"\"A pitch inventory is a scalewise list of the tones used in a composition or section thereof. \n",
    "For purposes of organization, the pitch inventories in this text always begin with the pitch \n",
    "A. Many students will have no need to prepare a pitch inventory, but for those students \n",
    "who have yet to develop a “hearing eye” that would allow instantaneous recognition of \n",
    "keys and tonal centers, a pitch inventory may be a necessity. A pitch inventory permits \n",
    "quick assessment of the selected pitches without prejudice to key or tonality. From there \n",
    "you can make a fairly accurate determination of key by observing the location of half and \n",
    "whole steps, accidentals such as raised sevenths, etc., and particular notes of the melody \n",
    "that are emphasized.\"\"\"\n",
    "# model.ask(question,instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a39c7fd-6df3-4a9c-a48b-b2c9dca3cf13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is a pitch inventory?\n",
      "Answer: A pitch inventory is a scale-wise list of the tones used in a composition or section thereof.\n",
      "2. Who might need to prepare a pitch inventory?\n",
      "Answer: Students who have not developed a \"hearing eye\" that allows them to recognize keys and tonal centers may need to prepare a pitch inventory.\n",
      "3. Why do some students need a pitch inventory?\n",
      "Answer: A pitch inventory allows quick assessment of the selected pitches without prejudice to key or tonality, which can help students determine key more accurately.\n",
      "4. How does a pitch inventory help determine key?\n",
      "Answer: By observing the location of half and whole steps, accidentals such as raised sevenths, and particular notes of the melody that are emphasized, a pitch inventory can provide insight into the key of a piece of music.\n",
      "5. Is preparing a pitch inventory necessary for all students?\n",
      "Answer: No, many students will not need to prepare a pitch inventory as they already have a \"hearing eye\" that allows them to recognize keys and tonal centers.\n"
     ]
    }
   ],
   "source": [
    "qa_list = generate_qa_pairs(model,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124965d-e017-4807-84ff-0bc46025a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_corpus_for_qa_questions(corpus,model):\n",
    "#     chunks = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18c0e40d-f163-4bd2-8c8f-cb86c26030e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is a pitch inventory?',\n",
       "  'answer': 'A pitch inventory is a scale-wise list of the tones used in a composition or section thereof.'},\n",
       " {'question': 'Who might need to prepare a pitch inventory?',\n",
       "  'answer': 'Students who have not developed a \"hearing eye\" that allows them to recognize keys and tonal centers may need to prepare a pitch inventory.'},\n",
       " {'question': 'Why do some students need a pitch inventory?',\n",
       "  'answer': 'A pitch inventory allows quick assessment of the selected pitches without prejudice to key or tonality, which can help students determine key more accurately.'},\n",
       " {'question': 'How does a pitch inventory help determine key?',\n",
       "  'answer': 'By observing the location of half and whole steps, accidentals such as raised sevenths, and particular notes of the melody that are emphasized, a pitch inventory can provide insight into the key of a piece of music.'},\n",
       " {'question': 'Is preparing a pitch inventory necessary for all students?',\n",
       "  'answer': 'No, many students will not need to prepare a pitch inventory as they already have a \"hearing eye\" that allows them to recognize keys and tonal centers.'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d7c0132-501c-439b-9170-7ef7e5d6e6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846d142a-0c84-43ae-aa8a-80bcb23d5efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a pitch inventory?\n",
      "A pitch inventory is a scale-wise list of the tones used in a composition or section thereof. It allows quick assessment of the selected pitches without prejudice to key or tonality.\n",
      "\n",
      "Who might need to prepare a pitch inventory?\n",
      "Students who have not developed a \"hearing eye\" that allows them to instantly recognize keys and tonal centers may need to prepare a pitch inventory.\n",
      "\n",
      "What does a pitch inventory help determine?\n",
      "A pitch inventory helps determine the key of a composition by observing the location of half and whole steps, accidentals such as raised sevenths, and particular notes of the melody that are emphasized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928a4d44-bba8-4677-9237-1ca54a54f5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,GPTQConfig, pipeline,TextStreamer\n",
    "\n",
    "\n",
    "def format_prompt_mistral(prompt):\n",
    "    return f\"\"\"<s>[INST] {prompt} [/INST]\"\"\"\n",
    "\n",
    "\n",
    "def get_model_mistral():\n",
    "    model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "    # To use a different branch, change revision\n",
    "    # For example: revision=\"main\"\n",
    "    quantization_config_loading = GPTQConfig(bits=4, use_exllama = False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                              \n",
    "                                              quantization_config=quantization_config_loading,\n",
    "                                              device_map=\"cuda\",\n",
    "                                              trust_remote_code=True,\n",
    "                                              revision=\"gptq-4bit-32g-actorder_True\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4225cc-91e8-44b9-877a-f297b1a11828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8ffbea-cd28-417e-943f-744a71b6f283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20042327fb1b41dea3f6bb74bae26fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)der_True/config.json:   0%|          | 0.00/962 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd87606333d048faa4d39c681b9722c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/4.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c4aea71fdd4eaa877732cb02d1541d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ef930613e040b09b8b7c0e16afa63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d62056bed34195883314af0c111afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6050b449caa4c8ca4ac5883ea7c3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae45522d6f3740298e22c5844ce08433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_mistral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7eb117db-0bf7-4227-ad7e-29eacf63ee06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "138e8a5c-ce1c-4b71-89e8-e24567e2bc60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diwali is the festival of lights, so let's combine that with our favorite pastime - cardio workouts! Here are some fun puns for you to try out:\n",
      "\n",
      "1. \"I was feeling down during my workout today, but then I remembered it's Diwali - time to light up your day!\"\n",
      "2. \"Want to get in shape for Diwali? Try these cardio moves inspired by the colorful celebrations.\"\n",
      "3. \"Cardio and Diwali go hand-in-hand. Just remember to keep up the pace or risk burning a wick!\"\n",
      "4. \"With Diwali around the corner, it's the perfect time to add a little sparkle to your workout routine.\"\n",
      "5. \"Don't forget to stretch before your Diwali cardio workout. It'll help prevent any injuries from lighting too many candles.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_instruction(\n",
    "    prompt:str,\n",
    "    instruction:str = 'Think carefully and answer the given question as truthfully as possible',\n",
    "    llm_template = None):\n",
    "    instruction_format = f\"\"\"### Instruction: {instruction}:\n",
    "    \n",
    "### Input:\n",
    "Question: {prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    if llm_template:\n",
    "        return llm_template(instruction_format)\n",
    "    else:\n",
    "        return instruction_format\n",
    "    \n",
    "    \n",
    "def ask_model(model_pipeline, prompt,instruction=None, llm_template = None):\n",
    "    formatted_question = generate_instruction(prompt=prompt,instruction=instruction,llm_template=llm_template)\n",
    "    return model_pipeline(formatted_question)[0]['generated_text'].split('<s>')[0]\n",
    "    \n",
    "    \n",
    "instruction = \"Think and Talk like a copywriter and answer the following question\"\n",
    "query = \"Combine the spirit of diwali and cardio workouts to create puns\"\n",
    "\n",
    "\n",
    "print(ask_model(pipe,query,instruction = instruction, llm_template = format_prompt_mistral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5970bfd9-9510-474b-8be1-cc08eb280317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075f537-c6c0-44d6-9f3c-7ed511bfec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734d142-80a0-433a-89ed-6e14f8b27b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
