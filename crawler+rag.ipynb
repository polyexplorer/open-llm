{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 10:34:08,594 - INFO - Crawling: https://raag-hindustani.com/\n",
      "2024-09-27 10:34:11,246 - INFO - Crawling: https://raag-hindustani.com/Introduction.html\n",
      "2024-09-27 10:34:16,506 - INFO - Crawling: https://raag-hindustani.com/Notes.html\n",
      "2024-09-27 10:34:19,094 - INFO - Crawling: https://raag-hindustani.com/Scales1.html\n",
      "2024-09-27 10:34:23,488 - INFO - Crawling: https://raag-hindustani.com/Scales2.html\n",
      "2024-09-27 10:34:27,144 - INFO - Crawling: https://raag-hindustani.com/Scales3.html\n",
      "2024-09-27 10:34:29,827 - INFO - Crawling: https://raag-hindustani.com/Scales4.html\n",
      "2024-09-27 10:34:34,076 - INFO - Crawling: https://raag-hindustani.com/Scales5.html\n",
      "2024-09-27 10:34:38,262 - INFO - Crawling: https://raag-hindustani.com/Rhythm.html\n",
      "2024-09-27 10:34:41,569 - INFO - Crawling: https://raag-hindustani.com/Embellishment.html\n",
      "2024-09-27 10:34:44,375 - INFO - Crawling: https://raag-hindustani.com/Notation.html\n",
      "2024-09-27 10:34:47,235 - INFO - Crawling: https://raag-hindustani.com/SimpleSongs.html\n",
      "2024-09-27 10:34:53,564 - INFO - Crawling: https://raag-hindustani.com/Improvisation.html\n",
      "2024-09-27 10:34:59,055 - INFO - Crawling: https://raag-hindustani.com/Performance.html\n",
      "2024-09-27 10:35:01,397 - INFO - Crawling: https://raag-hindustani.com/LearningTools.html\n",
      "2024-09-27 10:35:05,446 - INFO - Crawling: https://raag-hindustani.com/Credits.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from queue import Queue\n",
    "import random\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "from requests.exceptions import RequestException\n",
    "import markdownify\n",
    "from urllib.parse import urljoin\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class WebCrawler:\n",
    "    def __init__(self, max_depth=2, delay=2):\n",
    "        self.current_url = None\n",
    "        self.max_depth = max_depth\n",
    "        self.delay = delay\n",
    "        self.visited = set()\n",
    "        self.queue = Queue()\n",
    "        self.rate_limit = 10  # Requests per second\n",
    "        self.last_request_time = time.time()\n",
    "        self.base_dir = \"/home/ubuntu/open-llm/crawler_dump\"\n",
    "\n",
    "    def wait_for_rate_limit(self):\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - self.last_request_time\n",
    "        if elapsed_time < 1 / self.rate_limit:\n",
    "            sleep_time = 1 / self.rate_limit - elapsed_time\n",
    "            time.sleep(sleep_time)\n",
    "        self.last_request_time = current_time\n",
    "\n",
    "    def is_allowed(self, url):\n",
    "        robots_txt = requests.get(f\"{url}/robots.txt\").text\n",
    "        allowed_domains = []\n",
    "        for line in robots_txt.split('\\n'):\n",
    "            if line.startswith(\"Allow:\") or line.startswith(\"User-agent: *\"):\n",
    "                allowed_domains.append(line.split()[1])\n",
    "        \n",
    "        return '*' in allowed_domains or any(domain in allowed_domains for domain in self.visited)\n",
    "\n",
    "    def wait_between_requests(self):\n",
    "        \"\"\"Random delay between requests\"\"\"\n",
    "        time.sleep(random.uniform(1, 5))\n",
    "\n",
    "    def convert_html_to_markdown(self, html_content):\n",
    "        \"\"\"Convert HTML content to markdown\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        # Convert to markdown\n",
    "        converter = markdownify.MarkdownConverter()\n",
    "        markdown_text = converter.convert(text)\n",
    "        \n",
    "        return markdown_text\n",
    "\n",
    "    def save_page(self, url, content):\n",
    "        \"\"\"Save the page content as markdown\"\"\"\n",
    "        dir_path = os.path.join(self.base_dir, *url.split('/')[2:]).replace(\".html\",\"\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Save the markdown file\n",
    "        md_file = os.path.join(dir_path, f\"{os.path.basename(url)}.md\")\n",
    "        with open(md_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(self.convert_html_to_markdown(content))\n",
    "\n",
    "    def save_page_with_images(self, url, content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Save the HTML file\n",
    "        self.save_page(url, content)\n",
    "        \n",
    "        # Find all images\n",
    "        images = soup.find_all('img')\n",
    "        for img in images:\n",
    "            src = img.get('src')\n",
    "            if src and src.startswith(('http://', 'https://')):\n",
    "                image_url = src\n",
    "            elif src.startswith('/'):\n",
    "                # Assume relative path, join with base URL\n",
    "                image_url = urljoin(url, src)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Extract filename from URL\n",
    "            filename = os.path.basename(image_url)\n",
    "            \n",
    "            # Save the image\n",
    "            image_dir = os.path.join(self.base_dir, *image_url.split('/')[2:])\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            response = requests.get(image_url)\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "\n",
    "    def crawl(self, url):\n",
    "        try:\n",
    "        #     if not self.is_allowed(url):\n",
    "        #         logging.warning(f\"Not allowed to crawl: {url}\")\n",
    "        #         return\n",
    "            if url not in self.visited and url != self.current_url:\n",
    "                self.visited.add(url)\n",
    "                self.current_url = url\n",
    "                logging.info(f\"Crawling: {url}\")\n",
    "                \n",
    "                response = requests.get(url, timeout=10, allow_redirects=True)\n",
    "                response.raise_for_status()  # Raise an exception for bad status codes\n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                links = soup.find_all('a')\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    # Convert relative URLs to absolute URLs\n",
    "                    if not href.startswith(('http://', 'https://')):\n",
    "                        href = urljoin(url, href)\n",
    "\n",
    "                    # logging.info(f\"HREF:{href}\")\n",
    "                    \n",
    "                    # Add url to Queue\n",
    "                    self.queue.put(href)\n",
    "                    self.save_page(url, response.text)\n",
    "                \n",
    "                self.wait_between_requests()\n",
    "                \n",
    "                while not self.queue.empty() and self.max_depth > 0:\n",
    "                    new_url = self.queue.get()\n",
    "                    self.crawl(new_url)\n",
    "                    self.max_depth -= 1\n",
    "            \n",
    "                # Save the current page\n",
    "                \n",
    "            \n",
    "        except RequestException as e:\n",
    "            logging.error(f\"Error fetching {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Unexpected error while crawling {url}\")\n",
    "\n",
    "    \n",
    "    async def crawl_async(self, url):\n",
    "        if not self.is_allowed(url):\n",
    "            print(f\"Not allowed to crawl: {url}\")\n",
    "            return\n",
    "        \n",
    "        if url not in self.visited:\n",
    "            self.visited.add(url)\n",
    "            print(f\"Crawling: {url}\")\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                try:\n",
    "                    async with session.get(url, timeout=10) as response:\n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        html = await response.text()\n",
    "                        soup = BeautifulSoup(html, 'html.parser')\n",
    "                        \n",
    "                        links = soup.find_all('a')\n",
    "                        for link in links:\n",
    "                            href = link.get('href')\n",
    "                            if href and href.startswith('http'):\n",
    "                                self.queue.put(href)\n",
    "                    \n",
    "                    await self.wait_between_requests()\n",
    "                    \n",
    "                    if self.queue.qsize() > 0 and self.max_depth > 0:\n",
    "                        url_to_crawl = self.queue.get()\n",
    "                        await self.crawl_async(url_to_crawl)\n",
    "                        self.max_depth -= 1\n",
    "                \n",
    "                except aiohttp.ClientError as e:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error while crawling {url}: {e}\")\n",
    "\n",
    "    async def start_crawling(self, initial_url):\n",
    "        self.queue.put(initial_url)\n",
    "        await self.crawl_async(initial_url)\n",
    "\n",
    "\n",
    "# Usage\n",
    "crawler = WebCrawler(max_depth=4)\n",
    "crawler.crawl('https://raag-hindustani.com/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Scales3/Scales3.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Scales3.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/LearningTools/LearningTools.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/LearningTools.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Scales4/Scales4.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Scales4.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Improvisation/Improvisation.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Improvisation.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Scales2/Scales2.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Scales2.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Introduction/Introduction.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Introduction.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Embellishment/Embellishment.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Embellishment.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Scales1/Scales1.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Scales1.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Performance/Performance.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Performance.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Scales5/Scales5.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Scales5.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/SimpleSongs/SimpleSongs.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/SimpleSongs.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Notes/Notes.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Notes.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Notation/Notation.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Notation.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Credits/Credits.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Credits.html.md\n",
      "Copied: /home/ubuntu/open-llm/crawler_dump/raag-hindustani.com/Rhythm/Rhythm.html.md -> /home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed/Rhythm.html.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_md_files(source_folder, target_folder):\n",
    "    # Ensure the target folder exists\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all files in the source folder and its subfolders\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):  # Check if it's an .md file\n",
    "                source_path = os.path.join(root, file)\n",
    "                target_path = os.path.join(target_folder, file)\n",
    "                \n",
    "                # Copy the .md file to the target folder\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                print(f\"Copied: {source_path} -> {target_path}\")\n",
    "copy_md_files(source_folder=\"/home/ubuntu/open-llm/crawler_dump/raag-hindustani.com\",target_folder=\"/home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import mysql.connector\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "genrative_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "embedding_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "CONNECT_TIMEOUT = 1000\n",
    "READ_TIMEOUT = 1000\n",
    "BOTO_SERVICE = \"bedrock-runtime\"\n",
    "REGION = \"us-east-1\"\n",
    "ENDPOINT_URL = \"https://bedrock-runtime.us-east-1.amazonaws.com\"\n",
    "\n",
    "\n",
    "boto3_client = boto3.client(\n",
    "    BOTO_SERVICE,\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    region_name=os.getenv(\"REGION\"),\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    ")\n",
    "\n",
    "\n",
    "llm_genrative = BedrockChat(\n",
    "    model_id=genrative_model_id,\n",
    "    client=boto3_client,\n",
    ")\n",
    "\n",
    "llm_embedding = BedrockEmbeddings(\n",
    "    model_id=embedding_model_id, client=boto3_client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "markdown_dir = \"/home/ubuntu/open-llm/crawler_dump/raag-hindustani-parsed\"\n",
    "docs = []\n",
    "for file in os.listdir(markdown_dir):\n",
    "    loader = UnstructuredMarkdownLoader(os.path.join(markdown_dir,file),mode=\"elements\")\n",
    "    data = loader.load()\n",
    "    docs.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NarrativeText'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khatka\n",
      "Andolan\n",
      "An andolan is a slow oscillation applied to a note. It usually features in ragas that use microtones and is applied to the notes in those ragas that involve the use of microtones. Microtones are unstable pitches that fall between two notes and are difficult to sustain. Artists can use this natural instability to their advantage by mastering the andolan and oscillating the note in a controlled fashion, rather like a graceful tight-rope walker.\n",
      "Andolan\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = [doc[0].page_content for doc in docs]\n",
    "splits = text_splitter.create_documents(texts)\n",
    "print(splits[27].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(splits, llm_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.8/site-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, the notes of raga Yaman are not explicitly stated. However, we can infer that Yaman uses the sharp variant of the fourth note (Ma or M) in the Indian classical music scale. The context mentions that Yaman belongs to the Kalyan scale and uses a key signature with the G-major scale when notating compositions in this raga. But it is important to note that Yaman is not the same as the G-major scale, as it begins on the note C and incorporates the sharpened fourth note.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = db.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_genrative\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What are the notes of raga yaman?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
